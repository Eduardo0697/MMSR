{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6538db3a-12f5-4a4f-b5cb-e43d824d493e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import datatable as dt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Variables that contains the file location\n",
    "from files import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaea0caa-e0a9-4176-9eaf-a37c81774924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'C:\\\\Users\\\\Richi\\\\Google Drive\\\\Study\\\\5th Semester\\\\Multimedia Search and Retrieval\\\\Task 1\\\\Git\\\\MMSR\\\\project\\\\functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we modify the file we need to reload it with this\n",
    "import importlib\n",
    "import functions #import the module here, so that it can be reloaded.\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e350e-d054-4adb-a464-e9e24ff41273",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab141-a7ed-45a9-8669-1151599f8530",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Song informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01097418-33c6-443c-8455-765c0471b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres\n",
    "genres  = dt.fread(file_genres_2).to_pandas()\n",
    "genres.set_index('id', inplace=True)\n",
    "# song infos (artist, song, album name)\n",
    "info  = dt.fread(file_info_2).to_pandas()\n",
    "info.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d318ed3-8c77-4846-b9bd-0ef6a260c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert genres to a list\n",
    "#genres['genre']= genres.genre.apply(lambda x: get_genres(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c359b95-cad8-4265-a2e6-2da461f75727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>['pop']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    genre\n",
       "id                       \n",
       "0009fFIM1eYThaPg  ['pop']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47510a01-2cff-46e0-a027-f27f54d1c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>album_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>Cheryl</td>\n",
       "      <td>Rain on Me</td>\n",
       "      <td>3 Words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  artist        song album_name\n",
       "id                                             \n",
       "0009fFIM1eYThaPg  Cheryl  Rain on Me    3 Words"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ba66-da3e-4ba9-b7ab-e50da53216d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lyrics based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3fa4d6-c8d6-41e7-9475-f26a558564a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lyrics based feature vectors\n",
    "tf_idf  = dt.fread(file_tfidf_2)\n",
    "tf_idf[dt.float64] = dt.float32\n",
    "tf_idf = tf_idf.to_pandas()\n",
    "tf_idf.set_index('id', inplace=True)\n",
    "\n",
    "word2vec  = dt.fread(file_word2vec_2, header=True)\n",
    "word2vec[dt.float64] = dt.float32\n",
    "word2vec = word2vec.to_pandas()\n",
    "word2vec.set_index('id', inplace=True)\n",
    "\n",
    "bert = dt.fread(file_bert_2,header=True)\n",
    "bert[dt.float64] = dt.float32\n",
    "bert = bert.to_pandas()\n",
    "bert.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf04da1-fdbc-4ad5-89b2-69a808a7f786",
   "metadata": {},
   "source": [
    "## Audio features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fca40-6396-47cf-b763-c76425ba652f",
   "metadata": {},
   "source": [
    "### [Low-level] Essentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56067efc-8ef1-43be-aba5-8284714dc49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.average_loudness</th>\n",
       "      <th>lowlevel.barkbands.mean_0</th>\n",
       "      <th>lowlevel.barkbands.mean_1</th>\n",
       "      <th>lowlevel.barkbands.mean_2</th>\n",
       "      <th>lowlevel.barkbands.mean_3</th>\n",
       "      <th>lowlevel.barkbands.mean_4</th>\n",
       "      <th>lowlevel.barkbands.mean_5</th>\n",
       "      <th>lowlevel.barkbands.mean_6</th>\n",
       "      <th>lowlevel.barkbands.mean_7</th>\n",
       "      <th>lowlevel.barkbands.mean_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.thpcp_30</th>\n",
       "      <th>tonal.thpcp_31</th>\n",
       "      <th>tonal.thpcp_32</th>\n",
       "      <th>tonal.thpcp_33</th>\n",
       "      <th>tonal.thpcp_34</th>\n",
       "      <th>tonal.thpcp_35</th>\n",
       "      <th>tonal.tuning_diatonic_strength</th>\n",
       "      <th>tonal.tuning_equal_tempered_deviation</th>\n",
       "      <th>tonal.tuning_frequency</th>\n",
       "      <th>tonal.tuning_nontempered_energy_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>0.933530</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752843</td>\n",
       "      <td>0.544831</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.372330</td>\n",
       "      <td>0.422420</td>\n",
       "      <td>0.855395</td>\n",
       "      <td>0.563373</td>\n",
       "      <td>0.226941</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.944264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010xmHR6UICBOYT</th>\n",
       "      <td>0.985564</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.078349</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119888</td>\n",
       "      <td>0.325183</td>\n",
       "      <td>0.882466</td>\n",
       "      <td>0.917528</td>\n",
       "      <td>0.421067</td>\n",
       "      <td>0.563627</td>\n",
       "      <td>0.500238</td>\n",
       "      <td>0.263457</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.981752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lowlevel.average_loudness  lowlevel.barkbands.mean_0  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.933530                   0.003060   \n",
       "0010xmHR6UICBOYT                   0.985564                   0.009209   \n",
       "\n",
       "                  lowlevel.barkbands.mean_1  lowlevel.barkbands.mean_2  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.015665                   0.004106   \n",
       "0010xmHR6UICBOYT                   0.078349                   0.004850   \n",
       "\n",
       "                  lowlevel.barkbands.mean_3  lowlevel.barkbands.mean_4  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.001265                   0.001988   \n",
       "0010xmHR6UICBOYT                   0.001458                   0.000622   \n",
       "\n",
       "                  lowlevel.barkbands.mean_5  lowlevel.barkbands.mean_6  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.001152                   0.000906   \n",
       "0010xmHR6UICBOYT                   0.000331                   0.001254   \n",
       "\n",
       "                  lowlevel.barkbands.mean_7  lowlevel.barkbands.mean_8  ...  \\\n",
       "id                                                                      ...   \n",
       "0009fFIM1eYThaPg                   0.000356                   0.001064  ...   \n",
       "0010xmHR6UICBOYT                   0.003589                   0.000960  ...   \n",
       "\n",
       "                  tonal.thpcp_30  tonal.thpcp_31  tonal.thpcp_32  \\\n",
       "id                                                                 \n",
       "0009fFIM1eYThaPg        0.752843        0.544831        0.394721   \n",
       "0010xmHR6UICBOYT        0.119888        0.325183        0.882466   \n",
       "\n",
       "                  tonal.thpcp_33  tonal.thpcp_34  tonal.thpcp_35  \\\n",
       "id                                                                 \n",
       "0009fFIM1eYThaPg        0.372330        0.422420        0.855395   \n",
       "0010xmHR6UICBOYT        0.917528        0.421067        0.563627   \n",
       "\n",
       "                  tonal.tuning_diatonic_strength  \\\n",
       "id                                                 \n",
       "0009fFIM1eYThaPg                        0.563373   \n",
       "0010xmHR6UICBOYT                        0.500238   \n",
       "\n",
       "                  tonal.tuning_equal_tempered_deviation  \\\n",
       "id                                                        \n",
       "0009fFIM1eYThaPg                               0.226941   \n",
       "0010xmHR6UICBOYT                               0.263457   \n",
       "\n",
       "                  tonal.tuning_frequency  \\\n",
       "id                                         \n",
       "0009fFIM1eYThaPg              434.193115   \n",
       "0010xmHR6UICBOYT              434.193115   \n",
       "\n",
       "                  tonal.tuning_nontempered_energy_ratio  \n",
       "id                                                       \n",
       "0009fFIM1eYThaPg                               0.944264  \n",
       "0010xmHR6UICBOYT                               0.981752  \n",
       "\n",
       "[2 rows x 1034 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essentia  = dt.fread(file_essentia)\n",
    "essentia[dt.float64] = dt.float32\n",
    "essentia = essentia.to_pandas()\n",
    "essentia.set_index('id', inplace=True)\n",
    "essentia.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3493c0b-6506-44d7-9f59-f3628cec9a27",
   "metadata": {},
   "source": [
    "### [Mid-level] BLF: Block-Level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0cfa4ff-feec-49af-96d5-fb4c3e91fa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BLF: Block-Level features\n",
    "\n",
    "blf_correlation  = dt.fread(file_blf_correlation)\n",
    "blf_correlation[dt.float64] = dt.float32\n",
    "blf_correlation = blf_correlation.to_pandas()\n",
    "blf_correlation.set_index('id', inplace=True)\n",
    "\n",
    "blf_deltaspectral  = dt.fread(file_blf_deltaspectral)\n",
    "blf_deltaspectral[dt.float64] = dt.float32\n",
    "blf_deltaspectral = blf_deltaspectral.to_pandas()\n",
    "blf_deltaspectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_spectral  = dt.fread(file_blf_spectral)\n",
    "blf_spectral[dt.float64] = dt.float32\n",
    "blf_spectral = blf_spectral.to_pandas()\n",
    "blf_spectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_spectralcontrast  = dt.fread(file_blf_spectralcontrast)\n",
    "blf_spectralcontrast[dt.float64] = dt.float32\n",
    "blf_spectralcontrast = blf_spectralcontrast.to_pandas()\n",
    "blf_spectralcontrast.set_index('id', inplace=True)\n",
    "\n",
    "blf_vardeltaspectral  = dt.fread(file_blf_vardeltaspectral)\n",
    "blf_vardeltaspectral[dt.float64] = dt.float32\n",
    "blf_vardeltaspectral = blf_vardeltaspectral.to_pandas()\n",
    "blf_vardeltaspectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_logfluc  = dt.fread(file_blf_logfluc)\n",
    "blf_logfluc[dt.float64] = dt.float32\n",
    "\n",
    "# This is done because in the csv it has an extra column name, \n",
    "# so in case someone with the original dataset tries to run it, it fixes that error\n",
    "# It looks weird, but it is because first i am loading the data into datatable and then pass it to dataframe\n",
    "new_cols = ['id']\n",
    "new_cols.extend(list(blf_logfluc.names[2:]))\n",
    "new_cols = tuple(new_cols)\n",
    "del blf_logfluc[:, -1]\n",
    "\n",
    "blf_logfluc.names = new_cols\n",
    "blf_logfluc = blf_logfluc.to_pandas()\n",
    "blf_logfluc.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeacc90-c862-48d6-831e-f3ff6868a0df",
   "metadata": {},
   "source": [
    "### [Mid-level] MFCC: Mel Frequency Cepstral Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8902ca71-a8d3-4336-83ec-415347a92637",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_bow  = dt.fread(file_mfcc_bow)\n",
    "mfcc_bow[dt.float64] = dt.float32\n",
    "mfcc_bow = mfcc_bow.to_pandas()\n",
    "mfcc_bow.set_index('id', inplace=True)\n",
    "\n",
    "mfcc_stats  = dt.fread(file_mfcc_stats)\n",
    "mfcc_stats[dt.float64] = dt.float32\n",
    "mfcc_stats = mfcc_stats.to_pandas()\n",
    "mfcc_stats.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af626c8-4f55-445b-a61f-7ff6baf9b16c",
   "metadata": {},
   "source": [
    "## Video features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da64fbc6-4d3a-41da-bb34-8af0aaeb5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "incp  = dt.fread(file_incp)\n",
    "incp[dt.float64] = dt.float32\n",
    "incp = incp.to_pandas()\n",
    "incp.set_index('id', inplace=True)\n",
    "\n",
    "resnet  = dt.fread(file_resnet)\n",
    "resnet[dt.float64] = dt.float32\n",
    "resnet = resnet.to_pandas()\n",
    "resnet.set_index('id', inplace=True)\n",
    "\n",
    "vgg19  = dt.fread(file_vgg19)\n",
    "vgg19[dt.float64] = dt.float32\n",
    "vgg19 = vgg19.to_pandas()\n",
    "vgg19.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9d4d7-d725-485e-a4aa-05c7dcb56423",
   "metadata": {},
   "source": [
    "# Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86b9d96-8689-4376-bef2-1f36a492fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA , KernelPCA, FastICA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectFpr\n",
    "import functools as ft\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70bc7a47-e234-42f6-8f27-5152323911f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# since to compute every set for the full dataset is to big we will take only a sample set to compute scores and check which set will be best.\\n# if we find a good set we can do it on the full dataset later. This Strategy should save some computation time.\\n# lets take 20%\\nsample_ids = info.sample(frac=0.2, random_state=101)\\nsample_ids\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# since to compute every set for the full dataset is to big we will take only a sample set to compute scores and check which set will be best.\n",
    "# if we find a good set we can do it on the full dataset later. This Strategy should save some computation time.\n",
    "# lets take 20%\n",
    "sample_ids = info.sample(frac=0.2, random_state=101)\n",
    "sample_ids\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a961840-4b6d-4ed7-bc0f-ffdb8cc5d195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# now we have sample ids and can take a sample from each dataframe. We will overwrite the original Dataset to save Memory.\\ntf_idf = tf_idf.loc[sample_ids.index.values]\\nword2vec = word2vec.loc[sample_ids.index.values]\\nbert = bert.loc[sample_ids.index.values]\\nessentia = essentia.loc[sample_ids.index.values]\\nblf_correlation = blf_correlation.loc[sample_ids.index.values]\\nblf_deltaspectral = blf_deltaspectral.loc[sample_ids.index.values]\\nblf_spectral = blf_spectral.loc[sample_ids.index.values]\\nblf_spectralcontrast = blf_spectralcontrast.loc[sample_ids.index.values]\\nblf_vardeltaspectral = blf_vardeltaspectral.loc[sample_ids.index.values]\\nblf_logfluc = blf_logfluc.loc[sample_ids.index.values]\\nmfcc_bow = mfcc_bow.loc[sample_ids.index.values]\\nmfcc_stats = mfcc_stats.loc[sample_ids.index.values]\\nincp = incp.loc[sample_ids.index.values]\\nresnet = resnet.loc[sample_ids.index.values]\\nvgg19 = vgg19.loc[sample_ids.index.values]\\n\\n\\n# just a check so each Dataframe have correct IDS\\nassert(np.all(sample_ids.index.values == tf_idf.index.values))\\nassert(np.all(sample_ids.index.values == word2vec.index.values))\\nassert(np.all(sample_ids.index.values == bert.index.values))\\nassert(np.all(sample_ids.index.values == essentia.index.values))\\nassert(np.all(sample_ids.index.values == blf_correlation.index.values))\\nassert(np.all(sample_ids.index.values == blf_deltaspectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_spectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_spectralcontrast.index.values))\\nassert(np.all(sample_ids.index.values == blf_vardeltaspectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_logfluc.index.values))\\nassert(np.all(sample_ids.index.values == mfcc_bow.index.values))\\nassert(np.all(sample_ids.index.values == mfcc_stats.index.values))\\nassert(np.all(sample_ids.index.values == incp.index.values))\\nassert(np.all(sample_ids.index.values == resnet.index.values))\\nassert(np.all(sample_ids.index.values == vgg19.index.values))\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# now we have sample ids and can take a sample from each dataframe. We will overwrite the original Dataset to save Memory.\n",
    "tf_idf = tf_idf.loc[sample_ids.index.values]\n",
    "word2vec = word2vec.loc[sample_ids.index.values]\n",
    "bert = bert.loc[sample_ids.index.values]\n",
    "essentia = essentia.loc[sample_ids.index.values]\n",
    "blf_correlation = blf_correlation.loc[sample_ids.index.values]\n",
    "blf_deltaspectral = blf_deltaspectral.loc[sample_ids.index.values]\n",
    "blf_spectral = blf_spectral.loc[sample_ids.index.values]\n",
    "blf_spectralcontrast = blf_spectralcontrast.loc[sample_ids.index.values]\n",
    "blf_vardeltaspectral = blf_vardeltaspectral.loc[sample_ids.index.values]\n",
    "blf_logfluc = blf_logfluc.loc[sample_ids.index.values]\n",
    "mfcc_bow = mfcc_bow.loc[sample_ids.index.values]\n",
    "mfcc_stats = mfcc_stats.loc[sample_ids.index.values]\n",
    "incp = incp.loc[sample_ids.index.values]\n",
    "resnet = resnet.loc[sample_ids.index.values]\n",
    "vgg19 = vgg19.loc[sample_ids.index.values]\n",
    "\n",
    "\n",
    "# just a check so each Dataframe have correct IDS\n",
    "assert(np.all(sample_ids.index.values == tf_idf.index.values))\n",
    "assert(np.all(sample_ids.index.values == word2vec.index.values))\n",
    "assert(np.all(sample_ids.index.values == bert.index.values))\n",
    "assert(np.all(sample_ids.index.values == essentia.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_correlation.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_deltaspectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_spectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_spectralcontrast.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_vardeltaspectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_logfluc.index.values))\n",
    "assert(np.all(sample_ids.index.values == mfcc_bow.index.values))\n",
    "assert(np.all(sample_ids.index.values == mfcc_stats.index.values))\n",
    "assert(np.all(sample_ids.index.values == incp.index.values))\n",
    "assert(np.all(sample_ids.index.values == resnet.index.values))\n",
    "assert(np.all(sample_ids.index.values == vgg19.index.values))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5711b-06a2-458e-87a9-1d5ca7e2eed9",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd85bd29-85ef-4f50-a1c6-d11fb374d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA feature selection, we can get only features so we still have 99% explained variance from the Data.\n",
    "def PCA_selection(df_set):\n",
    "    pca = PCA(n_components=0.99, svd_solver='auto')\n",
    "    PCA_features = pd.DataFrame(pca.fit_transform(df_set))\n",
    "    print (f' original shape: {df_set.shape}') \n",
    "    print (f' components = {pca.n_components_}, explained variance = {pca.explained_variance_ratio_.sum()}')\n",
    "    print (f' features reduced by {(1-pca.n_components_/df_set.shape[1]) *100:.2f}%')\n",
    "    return PCA_features\n",
    "\n",
    "# We can use also PCA Kernel but we need to fix the number of components. Here we have different things to try\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’}, default=’linear’\n",
    "def PCA_Kernel_selection(df_set, kernel='rbf', components:int=100, gamma=None):\n",
    "    kernel_pca = KernelPCA(n_components=components, kernel=kernel, gamma=gamma)\n",
    "    kernel_PCA_features = pd.DataFrame(kernel_pca.fit_transform(df_set))\n",
    "    return kernel_PCA_features\n",
    "\n",
    "# Fast ICA would be another solution of dimensionality reduction\n",
    "def ICA_selection(df_set, components:int=100):\n",
    "    fast_ICA = FastICA(n_components=components)\n",
    "    fast_ICA_features = pd.DataFrame(fast_ICA.fit_transform(df_set))\n",
    "    return fast_ICA_features\n",
    "\n",
    "# we could also try TSNE, but not sure about performance. maybe long computation time\n",
    "def TSNE_selection(df_set, components:int=100):\n",
    "    tsne = TSNE(n_components=components, learning_rate='auto', init='random')\n",
    "    tsne_features = pd.DataFrame(tsne.fit_transform(mid_low_features))\n",
    "    return tsne_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4446acbf-9425-4aee-8d81-6e61e446098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features for each category\n",
    "text_features = {'tf_idf': tf_idf, 'word2vec':word2vec, 'bert':bert}\n",
    "audio_features = {'essentia':essentia,\n",
    "                  'blf_correlation':blf_correlation,\n",
    "                  'blf_deltaspectral':blf_deltaspectral,\n",
    "                  'blf_spectral':blf_spectral,\n",
    "                  'blf_spectralcontrast':blf_spectralcontrast,\n",
    "                  'blf_vardeltaspectral':blf_vardeltaspectral,\n",
    "                  'blf_logfluc':blf_logfluc,\n",
    "                  'mfcc_bow':mfcc_bow,\n",
    "                  'mfcc_stats':mfcc_stats}\n",
    "video_features = {'incp':incp, 'resnet':resnet, 'vgg19':vgg19}\n",
    "all_features = {**text_features, **audio_features, **video_features}\n",
    "\n",
    "\n",
    "# since if we want to try all different combinations this would be to much to handle. there are thousends of different combinations we need to concentrate on some.\n",
    "# from previous task we know bert and resnet where the more important features. so lets take them and combine with different audio features.\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "set1 = flatten([['bert'], audio_features.keys(), ['resnet']]) # one set with 1 text, audio, 1 video\n",
    "set2 = flatten([['bert'], audio_features.keys()]) # one set with 1 text, audio, without video\n",
    "set3 = flatten([audio_features.keys(), ['resnet']]) # one set without text, audio, video\n",
    "set4 = flatten([['bert'], ['resnet']]) # one set with text, video\n",
    "set5 = flatten([['bert'], ['essentia'], ['resnet']])  # one set with text, essentia, video\n",
    "set6 = flatten([['bert'], ['blf_correlation'], ['resnet']])\n",
    "set7 = flatten([['bert'], ['blf_deltaspectral'], ['resnet']])\n",
    "set8 = flatten([['bert'], ['essentia', 'blf_logfluc'], ['resnet']])\n",
    "set9 = flatten([['bert'], ['essentia', 'blf_logfluc', 'mfcc_bow'], ['resnet']])\n",
    "set10 = flatten([['bert'], ['essentia', 'blf_logfluc', 'mfcc_bow']])\n",
    "\n",
    "allsets = [set1, set2, set3, set4, set5, set6, set7, set8, set9, set10]\n",
    "\n",
    "# function for join sets, for later use. we are limited on RAM so do it only when we need the dataset.\n",
    "def join_dfs(set_n, features):\n",
    "    fset = [features[x] for x in set_n]\n",
    "    return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e54896-cec3-4736-af30-f86ec4be08c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already a combination of 5 features give 126 different combinations. To much to Test all!\n",
    "result_list = list(itertools.combinations(audio_features.keys(), 5))\n",
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a33f894-fee7-444b-ac05-9ea9ad2221d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf\n",
      " original shape: (68641, 1000)\n",
      " components = 957, explained variance = 0.9901756877480314\n",
      " features reduced by 4.30%\n",
      "------------------\n",
      "word2vec\n",
      " original shape: (68641, 300)\n",
      " components = 270, explained variance = 0.990314907049147\n",
      " features reduced by 10.00%\n",
      "------------------\n",
      "bert\n",
      " original shape: (68641, 768)\n",
      " components = 525, explained variance = 0.990061538059662\n",
      " features reduced by 31.64%\n",
      "------------------\n",
      "essentia\n",
      " original shape: (68641, 1034)\n",
      " components = 2, explained variance = 0.9918565334205097\n",
      " features reduced by 99.81%\n",
      "------------------\n",
      "blf_correlation\n",
      " original shape: (68641, 1326)\n",
      " components = 960, explained variance = 0.9900035135747746\n",
      " features reduced by 27.60%\n",
      "------------------\n",
      "blf_deltaspectral\n",
      " original shape: (68641, 1372)\n",
      " components = 362, explained variance = 0.9900050860046592\n",
      " features reduced by 73.62%\n",
      "------------------\n",
      "blf_spectral\n",
      " original shape: (68641, 980)\n",
      " components = 147, explained variance = 0.9901046629066191\n",
      " features reduced by 85.00%\n",
      "------------------\n",
      "blf_spectralcontrast\n",
      " original shape: (68641, 800)\n",
      " components = 108, explained variance = 0.9900852007686157\n",
      " features reduced by 86.50%\n",
      "------------------\n",
      "blf_vardeltaspectral\n",
      " original shape: (68641, 1344)\n",
      " components = 177, explained variance = 0.99005190527436\n",
      " features reduced by 86.83%\n",
      "------------------\n",
      "blf_logfluc\n",
      " original shape: (68641, 3626)\n",
      " components = 554, explained variance = 0.9900149002868229\n",
      " features reduced by 84.72%\n",
      "------------------\n",
      "mfcc_bow\n",
      " original shape: (68641, 500)\n",
      " components = 388, explained variance = 0.9900054837042523\n",
      " features reduced by 22.40%\n",
      "------------------\n",
      "mfcc_stats\n",
      " original shape: (68641, 104)\n",
      " components = 76, explained variance = 0.9903480738159525\n",
      " features reduced by 26.92%\n",
      "------------------\n",
      "incp\n",
      " original shape: (68641, 4096)\n",
      " components = 2295, explained variance = 0.9900060283806268\n",
      " features reduced by 43.97%\n",
      "------------------\n",
      "resnet\n",
      " original shape: (68641, 4096)\n",
      " components = 1946, explained variance = 0.9900125487446361\n",
      " features reduced by 52.49%\n",
      "------------------\n",
      "vgg19\n",
      " original shape: (68641, 8192)\n",
      " components = 4377, explained variance = 0.9900061208000029\n",
      " features reduced by 46.57%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# lets see how much we can reduce by PCA if we still want to have 99% explained variance:\n",
    "pca_features={}\n",
    "for name, val in zip(all_features.keys(), all_features.values()):\n",
    "    print(name)\n",
    "    pca_features[name] = PCA_selection(val)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4a412-0cc2-4c97-bd34-27384a08008f",
   "metadata": {},
   "source": [
    "## Compute similarity and top IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5613187-532e-4a05-a7d9-6fe229c13614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ids(df, setname):\n",
    "    # generate similarity\n",
    "    result = compute_in_batches_distance(df.to_numpy(dtype=np.float32), df.to_numpy(dtype=np.float32), simfunction = get_cosine_similarity, batches=100)\n",
    "    print(result.shape)\n",
    "    np.fill_diagonal(result, -1)\n",
    "    # save to file\n",
    "    dt.Frame(pd.DataFrame(compute_in_batches_topIds(result,info.index.values,100,100), index=info.index.values).reset_index()).to_csv(f'./TopIdsTaskGenerated/top_ids_cosine_earlyfusion_{setname}_complete.csv')\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49fa080b-699e-43ee-bfe1-3f148fe88f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '136_x', '28_x', '17_x', '132_x', '107_x', '121_x', '76_x', '113_x', '111_x', '144_x', '16_x', '108_x', '95_x', '84_x', '19_x', '13_x', '117_x', '30_x', '88_x', '36_x', '42_x', '67_x', '112_x', '103_x', '92_x', '52_x', '124_x', '44_x', '109_x', '138_x', '10_x', '78_x', '105_x', '21_x', '143_x', '45_x', '97_x', '122_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '118_x', '18_x', '58_x', '73_x', '100_x', '51_x', '129_x', '26_x', '77_x', '102_x', '9_x', '65_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '116_x', '24_x', '40_x', '91_x', '25_x', '135_x', '146_x', '93_x', '11_x', '12_x', '48_x', '56_x', '123_x', '130_x', '80_x', '59_x', '71_x', '128_x', '106_x', '75_x', '126_x', '7_x', '137_x', '141_x', '64_x', '43_x', '37_x', '110_x', '104_x', '133_x', '94_x', '35_x', '140_x', '81_x', '31_x', '63_x', '50_x', '101_x', '70_x', '29_x', '54_x', '125_x', '33_x', '131_x', '145_x', '120_x', '134_x', '114_x', '69_x', '57_x', '98_x', '62_x', '96_x', '3_x', '83_x', '119_x', '139_x', '23_x', '115_x', '34_x', '20_x', '4_x', '55_x', '89_x', '82_x', '87_x', '2_x', '39_x', '32_x', '142_x', '27_x', '90_x', '79_x', '127_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '28_x', '17_x', '107_x', '168_x', '172_x', '152_x', '155_x', '76_x', '166_x', '16_x', '161_x', '95_x', '84_x', '19_x', '13_x', '169_x', '30_x', '88_x', '42_x', '36_x', '174_x', '67_x', '147_x', '103_x', '92_x', '52_x', '44_x', '10_x', '78_x', '105_x', '21_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '157_x', '156_x', '165_x', '9_x', '65_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '159_x', '163_x', '24_x', '40_x', '91_x', '25_x', '164_x', '93_x', '11_x', '12_x', '48_x', '56_x', '80_x', '59_x', '71_x', '106_x', '75_x', '7_x', '64_x', '162_x', '43_x', '149_x', '37_x', '160_x', '104_x', '94_x', '35_x', '150_x', '81_x', '31_x', '63_x', '50_x', '101_x', '158_x', '70_x', '154_x', '175_x', '29_x', '54_x', '33_x', '69_x', '57_x', '98_x', '62_x', '170_x', '96_x', '3_x', '83_x', '23_x', '173_x', '34_x', '148_x', '171_x', '20_x', '4_x', '55_x', '89_x', '151_x', '82_x', '87_x', '2_x', '39_x', '32_x', '27_x', '90_x', '79_x', '176_x', '153_x', '167_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'222_x', '250_x', '254_x', '285_x', '277_x', '305_x', '200_x', '127_x', '198_x', '357_x', '136_x', '132_x', '218_x', '354_x', '338_x', '273_x', '121_x', '320_x', '212_x', '290_x', '311_x', '113_x', '256_x', '265_x', '221_x', '230_x', '195_x', '275_x', '228_x', '111_x', '189_x', '144_x', '237_x', '224_x', '287_x', '242_x', '108_x', '333_x', '210_x', '187_x', '232_x', '312_x', '323_x', '282_x', '226_x', '344_x', '117_x', '207_x', '180_x', '295_x', '215_x', '238_x', '186_x', '266_x', '112_x', '257_x', '342_x', '348_x', '234_x', '182_x', '288_x', '124_x', '260_x', '178_x', '109_x', '138_x', '352_x', '251_x', '240_x', '268_x', '306_x', '143_x', '258_x', '262_x', '193_x', '122_x', '281_x', '334_x', '185_x', '328_x', '276_x', '332_x', '118_x', '303_x', '360_x', '129_x', '278_x', '289_x', '339_x', '209_x', '269_x', '307_x', '335_x', '319_x', '205_x', '229_x', '247_x', '235_x', '192_x', '270_x', '294_x', '315_x', '116_x', '252_x', '211_x', '341_x', '135_x', '220_x', '146_x', '296_x', '123_x', '177_x', '181_x', '314_x', '130_x', '128_x', '327_x', '313_x', '325_x', '321_x', '126_x', '213_x', '199_x', '283_x', '137_x', '255_x', '141_x', '227_x', '223_x', '293_x', '359_x', '110_x', '239_x', '322_x', '203_x', '204_x', '353_x', '133_x', '343_x', '206_x', '271_x', '0_x', '217_x', '216_x', '263_x', '279_x', '197_x', '286_x', '140_x', '299_x', '347_x', '337_x', '349_x', '351_x', '355_x', '225_x', '261_x', '304_x', '196_x', '194_x', '274_x', '125_x', '190_x', '231_x', '330_x', '300_x', '131_x', '145_x', '120_x', '284_x', '310_x', '208_x', '134_x', '114_x', '309_x', '358_x', '179_x', '308_x', '318_x', '350_x', '119_x', '361_x', '346_x', '297_x', '336_x', '139_x', '115_x', '267_x', '280_x', '248_x', '253_x', '340_x', '298_x', '326_x', '259_x', '201_x', '214_x', '317_x', '356_x', '245_x', '316_x', '272_x', '191_x', '219_x', '291_x', '246_x', '331_x', '292_x', '184_x', '301_x', '142_x', '244_x', '202_x', '241_x', '345_x', '324_x', '329_x', '233_x', '236_x', '188_x', '1_x', '302_x', '243_x', '183_x', '264_x', '249_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '374_x', '28_x', '17_x', '107_x', '168_x', '172_x', '152_x', '364_x', '387_x', '368_x', '76_x', '155_x', '373_x', '375_x', '166_x', '16_x', '161_x', '95_x', '84_x', '19_x', '13_x', '30_x', '169_x', '88_x', '36_x', '42_x', '174_x', '67_x', '147_x', '383_x', '103_x', '92_x', '52_x', '367_x', '44_x', '10_x', '78_x', '105_x', '21_x', '377_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '157_x', '156_x', '9_x', '65_x', '165_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '159_x', '24_x', '163_x', '40_x', '384_x', '91_x', '25_x', '164_x', '93_x', '11_x', '12_x', '48_x', '56_x', '371_x', '80_x', '59_x', '71_x', '370_x', '106_x', '381_x', '75_x', '362_x', '7_x', '64_x', '43_x', '162_x', '37_x', '149_x', '104_x', '160_x', '365_x', '94_x', '35_x', '150_x', '363_x', '81_x', '31_x', '63_x', '50_x', '101_x', '379_x', '158_x', '70_x', '154_x', '29_x', '175_x', '54_x', '33_x', '380_x', '69_x', '57_x', '98_x', '62_x', '170_x', '96_x', '3_x', '83_x', '385_x', '369_x', '23_x', '173_x', '34_x', '148_x', '171_x', '372_x', '20_x', '4_x', '55_x', '89_x', '151_x', '82_x', '87_x', '2_x', '39_x', '32_x', '376_x', '27_x', '366_x', '90_x', '382_x', '79_x', '176_x', '378_x', '386_x', '153_x', '167_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'277_x', '305_x', '408_x', '200_x', '60_x', '407_x', '41_x', '218_x', '448_x', '212_x', '406_x', '195_x', '333_x', '504_x', '187_x', '312_x', '117_x', '207_x', '36_x', '266_x', '444_x', '342_x', '182_x', '124_x', '44_x', '411_x', '109_x', '138_x', '262_x', '122_x', '281_x', '8_x', '6_x', '328_x', '276_x', '61_x', '18_x', '58_x', '360_x', '129_x', '51_x', '26_x', '522_x', '229_x', '235_x', '519_x', '66_x', '401_x', '116_x', '40_x', '439_x', '467_x', '220_x', '177_x', '487_x', '128_x', '478_x', '321_x', '518_x', '499_x', '213_x', '450_x', '503_x', '199_x', '141_x', '227_x', '524_x', '451_x', '204_x', '343_x', '206_x', '35_x', '217_x', '263_x', '400_x', '140_x', '50_x', '351_x', '196_x', '498_x', '447_x', '274_x', '465_x', '131_x', '145_x', '33_x', '310_x', '179_x', '308_x', '462_x', '350_x', '346_x', '510_x', '267_x', '34_x', '514_x', '55_x', '214_x', '317_x', '443_x', '272_x', '485_x', '331_x', '184_x', '516_x', '202_x', '446_x', '183_x', '249_x', '250_x', '254_x', '14_x', '5_x', '74_x', '426_x', '132_x', '354_x', '121_x', '338_x', '273_x', '320_x', '507_x', '256_x', '221_x', '111_x', '144_x', '189_x', '237_x', '416_x', '410_x', '442_x', '210_x', '430_x', '479_x', '344_x', '19_x', '463_x', '234_x', '512_x', '288_x', '428_x', '251_x', '508_x', '496_x', '45_x', '469_x', '185_x', '332_x', '68_x', '398_x', '427_x', '436_x', '73_x', '289_x', '470_x', '269_x', '505_x', '420_x', '520_x', '319_x', '205_x', '270_x', '9_x', '396_x', '493_x', '146_x', '11_x', '523_x', '123_x', '486_x', '327_x', '126_x', '475_x', '390_x', '137_x', '223_x', '359_x', '413_x', '133_x', '279_x', '299_x', '31_x', '347_x', '477_x', '355_x', '394_x', '225_x', '464_x', '261_x', '304_x', '455_x', '194_x', '231_x', '120_x', '284_x', '134_x', '424_x', '309_x', '358_x', '466_x', '62_x', '3_x', '119_x', '298_x', '326_x', '259_x', '316_x', '2_x', '191_x', '219_x', '471_x', '246_x', '301_x', '244_x', '236_x', '302_x', '452_x', '222_x', '285_x', '434_x', '46_x', '49_x', '28_x', '17_x', '435_x', '456_x', '265_x', '275_x', '461_x', '431_x', '224_x', '287_x', '108_x', '473_x', '513_x', '517_x', '30_x', '180_x', '215_x', '238_x', '186_x', '67_x', '425_x', '112_x', '409_x', '348_x', '468_x', '260_x', '178_x', '352_x', '433_x', '495_x', '143_x', '268_x', '391_x', '405_x', '388_x', '334_x', '488_x', '38_x', '118_x', '460_x', '339_x', '209_x', '335_x', '65_x', '247_x', '47_x', '521_x', '22_x', '395_x', '211_x', '252_x', '458_x', '341_x', '12_x', '56_x', '48_x', '314_x', '130_x', '313_x', '64_x', '37_x', '239_x', '322_x', '353_x', '271_x', '476_x', '216_x', '472_x', '197_x', '286_x', '441_x', '63_x', '337_x', '438_x', '29_x', '125_x', '54_x', '474_x', '69_x', '419_x', '318_x', '414_x', '139_x', '297_x', '248_x', '497_x', '4_x', '417_x', '356_x', '245_x', '403_x', '502_x', '418_x', '292_x', '27_x', '459_x', '233_x', '188_x', '243_x', '509_x', '404_x', '423_x', '13_x', '15_x', '432_x', '482_x', '198_x', '357_x', '53_x', '136_x', '491_x', '397_x', '501_x', '457_x', '290_x', '311_x', '113_x', '230_x', '228_x', '481_x', '242_x', '16_x', '412_x', '232_x', '323_x', '282_x', '226_x', '415_x', '392_x', '492_x', '440_x', '295_x', '494_x', '42_x', '257_x', '484_x', '52_x', '454_x', '10_x', '240_x', '306_x', '21_x', '258_x', '421_x', '193_x', '399_x', '303_x', '278_x', '307_x', '315_x', '192_x', '294_x', '402_x', '72_x', '24_x', '511_x', '25_x', '135_x', '296_x', '181_x', '59_x', '71_x', '325_x', '75_x', '480_x', '449_x', '500_x', '437_x', '7_x', '283_x', '255_x', '453_x', '293_x', '43_x', '110_x', '203_x', '506_x', '349_x', '429_x', '490_x', '70_x', '190_x', '330_x', '300_x', '208_x', '393_x', '114_x', '57_x', '422_x', '361_x', '389_x', '336_x', '115_x', '23_x', '280_x', '253_x', '340_x', '20_x', '201_x', '445_x', '39_x', '32_x', '291_x', '489_x', '142_x', '515_x', '241_x', '345_x', '324_x', '483_x', '329_x', '127_x', '264_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:28<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '136_x', '28_x', '17_x', '132_x', '107_x', '121_x', '76_x', '113_x', '111_x', '144_x', '16_x', '108_x', '95_x', '84_x', '19_x', '13_x', '117_x', '30_x', '88_x', '36_x', '42_x', '67_x', '112_x', '103_x', '92_x', '52_x', '124_x', '44_x', '109_x', '138_x', '10_x', '78_x', '105_x', '21_x', '143_x', '45_x', '97_x', '122_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '118_x', '18_x', '58_x', '73_x', '100_x', '51_x', '129_x', '26_x', '77_x', '102_x', '9_x', '65_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '116_x', '24_x', '40_x', '91_x', '25_x', '135_x', '146_x', '93_x', '11_x', '12_x', '48_x', '56_x', '123_x', '130_x', '80_x', '59_x', '71_x', '128_x', '106_x', '75_x', '126_x', '7_x', '137_x', '141_x', '64_x', '43_x', '37_x', '110_x', '104_x', '133_x', '94_x', '35_x', '140_x', '81_x', '31_x', '63_x', '50_x', '101_x', '70_x', '29_x', '54_x', '125_x', '33_x', '131_x', '145_x', '120_x', '134_x', '114_x', '69_x', '57_x', '98_x', '62_x', '96_x', '3_x', '83_x', '119_x', '139_x', '23_x', '115_x', '34_x', '20_x', '4_x', '55_x', '89_x', '82_x', '87_x', '2_x', '39_x', '32_x', '142_x', '27_x', '90_x', '79_x', '127_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '28_x', '17_x', '107_x', '168_x', '172_x', '152_x', '155_x', '76_x', '166_x', '16_x', '161_x', '95_x', '84_x', '19_x', '13_x', '169_x', '30_x', '88_x', '42_x', '36_x', '174_x', '67_x', '147_x', '103_x', '92_x', '52_x', '44_x', '10_x', '78_x', '105_x', '21_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '157_x', '156_x', '165_x', '9_x', '65_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '159_x', '163_x', '24_x', '40_x', '91_x', '25_x', '164_x', '93_x', '11_x', '12_x', '48_x', '56_x', '80_x', '59_x', '71_x', '106_x', '75_x', '7_x', '64_x', '162_x', '43_x', '149_x', '37_x', '160_x', '104_x', '94_x', '35_x', '150_x', '81_x', '31_x', '63_x', '50_x', '101_x', '158_x', '70_x', '154_x', '175_x', '29_x', '54_x', '33_x', '69_x', '57_x', '98_x', '62_x', '170_x', '96_x', '3_x', '83_x', '23_x', '173_x', '34_x', '148_x', '171_x', '20_x', '4_x', '55_x', '89_x', '151_x', '82_x', '87_x', '2_x', '39_x', '32_x', '27_x', '90_x', '79_x', '176_x', '153_x', '167_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'222_x', '250_x', '254_x', '285_x', '277_x', '305_x', '200_x', '127_x', '198_x', '357_x', '136_x', '132_x', '218_x', '354_x', '338_x', '273_x', '121_x', '320_x', '212_x', '290_x', '311_x', '113_x', '256_x', '265_x', '221_x', '230_x', '195_x', '275_x', '228_x', '111_x', '189_x', '144_x', '237_x', '224_x', '287_x', '242_x', '108_x', '333_x', '210_x', '187_x', '232_x', '312_x', '323_x', '282_x', '226_x', '344_x', '117_x', '207_x', '180_x', '295_x', '215_x', '238_x', '186_x', '266_x', '112_x', '257_x', '342_x', '348_x', '234_x', '182_x', '288_x', '124_x', '260_x', '178_x', '109_x', '138_x', '352_x', '251_x', '240_x', '268_x', '306_x', '143_x', '258_x', '262_x', '193_x', '122_x', '281_x', '334_x', '185_x', '328_x', '276_x', '332_x', '118_x', '303_x', '360_x', '129_x', '278_x', '289_x', '339_x', '209_x', '269_x', '307_x', '335_x', '319_x', '205_x', '229_x', '247_x', '235_x', '192_x', '270_x', '294_x', '315_x', '116_x', '252_x', '211_x', '341_x', '135_x', '220_x', '146_x', '296_x', '123_x', '177_x', '181_x', '314_x', '130_x', '128_x', '327_x', '313_x', '325_x', '321_x', '126_x', '213_x', '199_x', '283_x', '137_x', '255_x', '141_x', '227_x', '223_x', '293_x', '359_x', '110_x', '239_x', '322_x', '203_x', '204_x', '353_x', '133_x', '343_x', '206_x', '271_x', '0_x', '217_x', '216_x', '263_x', '279_x', '197_x', '286_x', '140_x', '299_x', '347_x', '337_x', '349_x', '351_x', '355_x', '225_x', '261_x', '304_x', '196_x', '194_x', '274_x', '125_x', '190_x', '231_x', '330_x', '300_x', '131_x', '145_x', '120_x', '284_x', '310_x', '208_x', '134_x', '114_x', '309_x', '358_x', '179_x', '308_x', '318_x', '350_x', '119_x', '361_x', '346_x', '297_x', '336_x', '139_x', '115_x', '267_x', '280_x', '248_x', '253_x', '340_x', '298_x', '326_x', '259_x', '201_x', '214_x', '317_x', '356_x', '245_x', '316_x', '272_x', '191_x', '219_x', '291_x', '246_x', '331_x', '292_x', '184_x', '301_x', '142_x', '244_x', '202_x', '241_x', '345_x', '324_x', '329_x', '233_x', '236_x', '188_x', '1_x', '302_x', '243_x', '183_x', '264_x', '249_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '374_x', '28_x', '17_x', '107_x', '168_x', '172_x', '152_x', '364_x', '387_x', '368_x', '76_x', '155_x', '373_x', '375_x', '166_x', '16_x', '161_x', '95_x', '84_x', '19_x', '13_x', '30_x', '169_x', '88_x', '36_x', '42_x', '174_x', '67_x', '147_x', '383_x', '103_x', '92_x', '52_x', '367_x', '44_x', '10_x', '78_x', '105_x', '21_x', '377_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '157_x', '156_x', '9_x', '65_x', '165_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '159_x', '24_x', '163_x', '40_x', '384_x', '91_x', '25_x', '164_x', '93_x', '11_x', '12_x', '48_x', '56_x', '371_x', '80_x', '59_x', '71_x', '370_x', '106_x', '381_x', '75_x', '362_x', '7_x', '64_x', '43_x', '162_x', '37_x', '149_x', '104_x', '160_x', '365_x', '94_x', '35_x', '150_x', '363_x', '81_x', '31_x', '63_x', '50_x', '101_x', '379_x', '158_x', '70_x', '154_x', '29_x', '175_x', '54_x', '33_x', '380_x', '69_x', '57_x', '98_x', '62_x', '170_x', '96_x', '3_x', '83_x', '385_x', '369_x', '23_x', '173_x', '34_x', '148_x', '171_x', '372_x', '20_x', '4_x', '55_x', '89_x', '151_x', '82_x', '87_x', '2_x', '39_x', '32_x', '376_x', '27_x', '366_x', '90_x', '382_x', '79_x', '176_x', '378_x', '386_x', '153_x', '167_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [01:39<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:20<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '28_x', '17_x', '107_x', '76_x', '16_x', '95_x', '84_x', '19_x', '13_x', '30_x', '88_x', '36_x', '42_x', '67_x', '103_x', '92_x', '52_x', '44_x', '10_x', '78_x', '105_x', '21_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '9_x', '65_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '24_x', '40_x', '91_x', '25_x', '93_x', '11_x', '12_x', '48_x', '56_x', '80_x', '59_x', '71_x', '106_x', '75_x', '7_x', '64_x', '43_x', '37_x', '104_x', '94_x', '35_x', '81_x', '31_x', '63_x', '50_x', '101_x', '70_x', '29_x', '54_x', '33_x', '69_x', '57_x', '98_x', '62_x', '96_x', '3_x', '83_x', '23_x', '34_x', '20_x', '4_x', '55_x', '89_x', '82_x', '87_x', '2_x', '39_x', '32_x', '27_x', '90_x', '79_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'135_x', '131_x', '145_x', '120_x', '146_x', '124_x', '134_x', '123_x', '114_x', '109_x', '130_x', '138_x', '128_x', '143_x', '136_x', '119_x', '126_x', '122_x', '132_x', '139_x', '121_x', '115_x', '137_x', '141_x', '113_x', '110_x', '111_x', '118_x', '144_x', '133_x', '129_x', '0_x', '108_x', '1_x', '140_x', '142_x', '117_x', '116_x', '127_x', '125_x', '112_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'14_x', '5_x', '46_x', '74_x', '49_x', '60_x', '41_x', '53_x', '28_x', '17_x', '107_x', '168_x', '172_x', '152_x', '76_x', '155_x', '166_x', '16_x', '161_x', '95_x', '84_x', '19_x', '13_x', '30_x', '169_x', '88_x', '36_x', '42_x', '174_x', '67_x', '147_x', '103_x', '92_x', '52_x', '44_x', '10_x', '78_x', '105_x', '21_x', '45_x', '97_x', '86_x', '8_x', '6_x', '61_x', '68_x', '38_x', '18_x', '58_x', '73_x', '100_x', '51_x', '26_x', '77_x', '102_x', '157_x', '156_x', '9_x', '65_x', '165_x', '47_x', '22_x', '99_x', '85_x', '66_x', '72_x', '159_x', '24_x', '163_x', '40_x', '91_x', '25_x', '164_x', '93_x', '11_x', '12_x', '48_x', '56_x', '80_x', '59_x', '71_x', '106_x', '75_x', '7_x', '64_x', '43_x', '162_x', '37_x', '149_x', '104_x', '160_x', '94_x', '35_x', '150_x', '81_x', '31_x', '63_x', '50_x', '101_x', '158_x', '70_x', '154_x', '29_x', '175_x', '54_x', '33_x', '69_x', '57_x', '98_x', '62_x', '170_x', '96_x', '3_x', '83_x', '23_x', '173_x', '34_x', '148_x', '171_x', '20_x', '4_x', '55_x', '89_x', '151_x', '82_x', '87_x', '2_x', '39_x', '32_x', '27_x', '90_x', '79_x', '176_x', '153_x', '167_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'222_x', '250_x', '254_x', '285_x', '277_x', '305_x', '200_x', '198_x', '357_x', '136_x', '132_x', '218_x', '121_x', '338_x', '273_x', '354_x', '320_x', '212_x', '290_x', '311_x', '113_x', '256_x', '265_x', '221_x', '111_x', '144_x', '195_x', '228_x', '230_x', '189_x', '275_x', '237_x', '224_x', '287_x', '242_x', '108_x', '1_x', '333_x', '210_x', '187_x', '232_x', '312_x', '323_x', '282_x', '226_x', '344_x', '117_x', '207_x', '180_x', '295_x', '215_x', '238_x', '186_x', '266_x', '112_x', '257_x', '342_x', '348_x', '234_x', '182_x', '288_x', '124_x', '260_x', '178_x', '109_x', '138_x', '352_x', '251_x', '240_x', '143_x', '268_x', '306_x', '258_x', '262_x', '193_x', '122_x', '281_x', '334_x', '185_x', '328_x', '276_x', '332_x', '118_x', '303_x', '360_x', '129_x', '278_x', '289_x', '339_x', '209_x', '269_x', '307_x', '335_x', '319_x', '205_x', '229_x', '235_x', '247_x', '192_x', '270_x', '294_x', '315_x', '116_x', '211_x', '252_x', '135_x', '341_x', '220_x', '146_x', '296_x', '123_x', '177_x', '181_x', '314_x', '130_x', '128_x', '327_x', '313_x', '126_x', '321_x', '325_x', '213_x', '137_x', '199_x', '283_x', '141_x', '255_x', '227_x', '223_x', '293_x', '359_x', '110_x', '239_x', '322_x', '203_x', '133_x', '204_x', '353_x', '343_x', '206_x', '271_x', '0_x', '217_x', '216_x', '263_x', '279_x', '197_x', '140_x', '286_x', '299_x', '347_x', '337_x', '349_x', '351_x', '355_x', '225_x', '261_x', '304_x', '196_x', '194_x', '125_x', '274_x', '190_x', '231_x', '330_x', '300_x', '131_x', '145_x', '120_x', '284_x', '310_x', '134_x', '208_x', '114_x', '309_x', '358_x', '179_x', '308_x', '318_x', '350_x', '119_x', '361_x', '139_x', '297_x', '336_x', '346_x', '115_x', '267_x', '280_x', '248_x', '253_x', '340_x', '298_x', '326_x', '259_x', '201_x', '214_x', '317_x', '356_x', '245_x', '316_x', '272_x', '191_x', '219_x', '291_x', '246_x', '331_x', '292_x', '184_x', '142_x', '301_x', '244_x', '202_x', '241_x', '345_x', '324_x', '329_x', '233_x', '236_x', '188_x', '127_x', '302_x', '243_x', '183_x', '264_x', '249_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'25_x', '14_x', '33_x', '5_x', '46_x', '52_x', '74_x', '11_x', '12_x', '48_x', '44_x', '56_x', '49_x', '60_x', '41_x', '57_x', '10_x', '59_x', '69_x', '71_x', '53_x', '62_x', '21_x', '3_x', '75_x', '45_x', '28_x', '17_x', '8_x', '15_x', '23_x', '7_x', '34_x', '6_x', '64_x', '61_x', '20_x', '43_x', '68_x', '38_x', '4_x', '55_x', '37_x', '18_x', '58_x', '73_x', '51_x', '26_x', '2_x', '16_x', '35_x', '39_x', '32_x', '31_x', '9_x', '50_x', '63_x', '65_x', '47_x', '19_x', '22_x', '27_x', '70_x', '30_x', '66_x', '72_x', '36_x', '24_x', '42_x', '40_x', '29_x', '67_x', '54_x', '13_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'374_x', '107_x', '168_x', '172_x', '152_x', '364_x', '387_x', '368_x', '76_x', '155_x', '373_x', '375_x', '166_x', '1_x', '161_x', '95_x', '84_x', '169_x', '88_x', '174_x', '147_x', '383_x', '103_x', '92_x', '367_x', '78_x', '105_x', '377_x', '97_x', '86_x', '100_x', '77_x', '102_x', '157_x', '156_x', '165_x', '99_x', '85_x', '159_x', '163_x', '384_x', '91_x', '164_x', '93_x', '371_x', '80_x', '370_x', '106_x', '381_x', '362_x', '162_x', '149_x', '104_x', '160_x', '365_x', '94_x', '0_x', '150_x', '363_x', '81_x', '379_x', '101_x', '158_x', '154_x', '175_x', '380_x', '98_x', '170_x', '96_x', '83_x', '385_x', '369_x', '173_x', '148_x', '171_x', '372_x', '89_x', '151_x', '82_x', '87_x', '376_x', '366_x', '90_x', '382_x', '79_x', '176_x', '378_x', '386_x', '153_x', '167_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [02:15<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:35<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:09<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:00<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:35<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:01<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:55<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'277_x', '305_x', '200_x', '60_x', '41_x', '218_x', '152_x', '364_x', '212_x', '195_x', '333_x', '187_x', '312_x', '117_x', '207_x', '169_x', '88_x', '36_x', '266_x', '342_x', '182_x', '124_x', '44_x', '109_x', '138_x', '262_x', '97_x', '122_x', '281_x', '8_x', '6_x', '328_x', '276_x', '61_x', '18_x', '58_x', '360_x', '51_x', '129_x', '26_x', '156_x', '165_x', '229_x', '235_x', '66_x', '116_x', '40_x', '384_x', '164_x', '220_x', '177_x', '128_x', '321_x', '213_x', '199_x', '141_x', '227_x', '204_x', '160_x', '343_x', '206_x', '35_x', '217_x', '263_x', '140_x', '379_x', '50_x', '351_x', '196_x', '274_x', '33_x', '131_x', '145_x', '310_x', '179_x', '308_x', '350_x', '385_x', '346_x', '267_x', '34_x', '55_x', '214_x', '317_x', '151_x', '272_x', '331_x', '184_x', '202_x', '90_x', '382_x', '378_x', '153_x', '183_x', '249_x', '250_x', '254_x', '14_x', '5_x', '74_x', '132_x', '107_x', '121_x', '168_x', '172_x', '273_x', '338_x', '320_x', '354_x', '76_x', '256_x', '375_x', '221_x', '111_x', '144_x', '189_x', '166_x', '237_x', '210_x', '95_x', '344_x', '19_x', '174_x', '147_x', '383_x', '234_x', '288_x', '251_x', '45_x', '185_x', '332_x', '68_x', '73_x', '289_x', '269_x', '157_x', '319_x', '9_x', '270_x', '205_x', '85_x', '163_x', '91_x', '146_x', '11_x', '123_x', '80_x', '106_x', '327_x', '126_x', '362_x', '137_x', '223_x', '359_x', '133_x', '279_x', '363_x', '31_x', '299_x', '347_x', '355_x', '158_x', '225_x', '261_x', '304_x', '194_x', '231_x', '120_x', '284_x', '134_x', '309_x', '358_x', '62_x', '3_x', '83_x', '119_x', '369_x', '171_x', '298_x', '326_x', '259_x', '82_x', '316_x', '2_x', '191_x', '219_x', '246_x', '301_x', '244_x', '366_x', '386_x', '236_x', '302_x', '222_x', '285_x', '46_x', '49_x', '28_x', '17_x', '387_x', '155_x', '265_x', '275_x', '224_x', '287_x', '108_x', '161_x', '30_x', '180_x', '215_x', '238_x', '186_x', '67_x', '112_x', '348_x', '92_x', '367_x', '260_x', '178_x', '352_x', '78_x', '105_x', '143_x', '268_x', '334_x', '38_x', '118_x', '100_x', '339_x', '209_x', '335_x', '65_x', '247_x', '47_x', '22_x', '99_x', '159_x', '211_x', '252_x', '341_x', '12_x', '56_x', '48_x', '314_x', '130_x', '381_x', '313_x', '64_x', '37_x', '239_x', '322_x', '104_x', '353_x', '365_x', '271_x', '150_x', '216_x', '197_x', '286_x', '63_x', '337_x', '29_x', '54_x', '125_x', '380_x', '69_x', '98_x', '318_x', '170_x', '139_x', '297_x', '173_x', '248_x', '372_x', '4_x', '356_x', '245_x', '87_x', '292_x', '27_x', '233_x', '188_x', '167_x', '243_x', '13_x', '15_x', '198_x', '357_x', '53_x', '374_x', '136_x', '368_x', '290_x', '311_x', '113_x', '373_x', '230_x', '228_x', '242_x', '16_x', '232_x', '323_x', '84_x', '282_x', '226_x', '42_x', '295_x', '257_x', '103_x', '52_x', '10_x', '240_x', '21_x', '306_x', '258_x', '377_x', '193_x', '86_x', '303_x', '278_x', '77_x', '102_x', '307_x', '315_x', '192_x', '294_x', '72_x', '24_x', '25_x', '135_x', '296_x', '93_x', '181_x', '371_x', '59_x', '71_x', '370_x', '75_x', '325_x', '7_x', '283_x', '255_x', '43_x', '162_x', '293_x', '110_x', '149_x', '203_x', '94_x', '81_x', '101_x', '349_x', '70_x', '154_x', '175_x', '190_x', '330_x', '300_x', '208_x', '114_x', '57_x', '96_x', '361_x', '336_x', '23_x', '115_x', '148_x', '280_x', '253_x', '340_x', '20_x', '201_x', '89_x', '39_x', '32_x', '291_x', '142_x', '376_x', '345_x', '241_x', '324_x', '329_x', '79_x', '176_x', '127_x', '264_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [01:43<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:57<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_39452\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "100%|██████████| 100/100 [00:49<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68641, 68641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:08<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# run for all sets\n",
    "for i, s in enumerate(allsets):\n",
    "    print(f'doing set{i+1}')\n",
    "    joined_df = join_dfs(s, pca_features)\n",
    "    generate_ids(joined_df, f'set{i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56505da6-4f67-40d1-a855-3aef79b47ab7",
   "metadata": {},
   "source": [
    "## Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e19f1f11-c635-4837-ae76-414ba3099e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP, RR, ndcg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:51<00:00, 401.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set1: (0.4713479340727472, 0.6048723570874691, 0.7041884452006223)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:51<00:00, 401.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set2: (0.4713474535656341, 0.604863519930501, 0.7041875188866166)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:52<00:00, 398.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set3: (0.4713393732284447, 0.6046845645422031, 0.7041883913613585)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:50<00:00, 403.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set4: (0.4664316371393523, 0.6024990755300572, 0.7013290695874996)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:51<00:00, 401.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set5: (0.46813708135545873, 0.6023039143638641, 0.7018791414237513)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:52<00:00, 397.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set6: (0.4664315549742861, 0.6024914443836024, 0.7013287307305869)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:49<00:00, 403.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set7: (0.4664364145023312, 0.6025497710122169, 0.7013350723798133)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:50<00:00, 403.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set8: (0.4713642087708921, 0.6053001877535551, 0.7042024195186465)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:48<00:00, 408.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set9: (0.4713593727148014, 0.605282841332436, 0.7042003438747417)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:48<00:00, 406.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics set10: (0.47135848672574293, 0.6052765217305164, 0.70419757729533)\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('AP, RR, ndcg')\n",
    "for i in range(1,11):\n",
    "    top_set = dt.fread(f'./TopIdsTaskGenerated/top_ids_cosine_earlyfusion_set{i}_complete.csv', header=True).to_pandas()\n",
    "    top_set.set_index('index', inplace=True)\n",
    "    print(f'Metrics set{i}:', getMetrics(top_set, 100, genres))\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e11df4-e3e7-488e-88e3-d859065d7674",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad43af-6228-4f97-9c03-83e443ee66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def normalize_df(df, post_processing_scaler):\n",
    "    df_np_values = df.values  #returns a numpy array\n",
    "    # scale\n",
    "    df_normalized = post_processing_scaler.fit_transform(df_np_values)\n",
    "    # reset index from original\n",
    "    df_normalized = pd.DataFrame(df_normalized).set_index(df.index)\n",
    "    df_normalized.columns = df.columns[:]\n",
    "    return df_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
