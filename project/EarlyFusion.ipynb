{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6538db3a-12f5-4a4f-b5cb-e43d824d493e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import datatable as dt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Variables that contains the file location\n",
    "from files import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aaea0caa-e0a9-4176-9eaf-a37c81774924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'C:\\\\Users\\\\Richi\\\\Google Drive\\\\Study\\\\5th Semester\\\\Multimedia Search and Retrieval\\\\Task 1\\\\Git\\\\MMSR\\\\project\\\\functions.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we modify the file we need to reload it with this\n",
    "import importlib\n",
    "import functions #import the module here, so that it can be reloaded.\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e350e-d054-4adb-a464-e9e24ff41273",
   "metadata": {},
   "source": [
    "# Data\n",
    "Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab141-a7ed-45a9-8669-1151599f8530",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Song informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01097418-33c6-443c-8455-765c0471b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres\n",
    "genres  = dt.fread(file_genres_2).to_pandas()\n",
    "genres.set_index('id', inplace=True)\n",
    "# song infos (artist, song, album name)\n",
    "info  = dt.fread(file_info_2).to_pandas()\n",
    "info.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d318ed3-8c77-4846-b9bd-0ef6a260c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert genres to a list\n",
    "#genres['genre']= genres.genre.apply(lambda x: get_genres(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c359b95-cad8-4265-a2e6-2da461f75727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>['pop']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    genre\n",
       "id                       \n",
       "0009fFIM1eYThaPg  ['pop']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47510a01-2cff-46e0-a027-f27f54d1c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>album_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>Cheryl</td>\n",
       "      <td>Rain on Me</td>\n",
       "      <td>3 Words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  artist        song album_name\n",
       "id                                             \n",
       "0009fFIM1eYThaPg  Cheryl  Rain on Me    3 Words"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ba66-da3e-4ba9-b7ab-e50da53216d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lyrics based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3fa4d6-c8d6-41e7-9475-f26a558564a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lyrics based feature vectors\n",
    "tf_idf  = dt.fread(file_tfidf_2)\n",
    "tf_idf[dt.float64] = dt.float32\n",
    "tf_idf = tf_idf.to_pandas()\n",
    "tf_idf.set_index('id', inplace=True)\n",
    "\n",
    "word2vec  = dt.fread(file_word2vec_2, header=True)\n",
    "word2vec[dt.float64] = dt.float32\n",
    "word2vec = word2vec.to_pandas()\n",
    "word2vec.set_index('id', inplace=True)\n",
    "\n",
    "bert = dt.fread(file_bert_2,header=True)\n",
    "bert[dt.float64] = dt.float32\n",
    "bert = bert.to_pandas()\n",
    "bert.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf04da1-fdbc-4ad5-89b2-69a808a7f786",
   "metadata": {},
   "source": [
    "## Audio features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fca40-6396-47cf-b763-c76425ba652f",
   "metadata": {},
   "source": [
    "### [Low-level] Essentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56067efc-8ef1-43be-aba5-8284714dc49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.average_loudness</th>\n",
       "      <th>lowlevel.barkbands.mean_0</th>\n",
       "      <th>lowlevel.barkbands.mean_1</th>\n",
       "      <th>lowlevel.barkbands.mean_2</th>\n",
       "      <th>lowlevel.barkbands.mean_3</th>\n",
       "      <th>lowlevel.barkbands.mean_4</th>\n",
       "      <th>lowlevel.barkbands.mean_5</th>\n",
       "      <th>lowlevel.barkbands.mean_6</th>\n",
       "      <th>lowlevel.barkbands.mean_7</th>\n",
       "      <th>lowlevel.barkbands.mean_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.thpcp_30</th>\n",
       "      <th>tonal.thpcp_31</th>\n",
       "      <th>tonal.thpcp_32</th>\n",
       "      <th>tonal.thpcp_33</th>\n",
       "      <th>tonal.thpcp_34</th>\n",
       "      <th>tonal.thpcp_35</th>\n",
       "      <th>tonal.tuning_diatonic_strength</th>\n",
       "      <th>tonal.tuning_equal_tempered_deviation</th>\n",
       "      <th>tonal.tuning_frequency</th>\n",
       "      <th>tonal.tuning_nontempered_energy_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fFIM1eYThaPg</th>\n",
       "      <td>0.933530</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752843</td>\n",
       "      <td>0.544831</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.372330</td>\n",
       "      <td>0.422420</td>\n",
       "      <td>0.855395</td>\n",
       "      <td>0.563373</td>\n",
       "      <td>0.226941</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.944264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0010xmHR6UICBOYT</th>\n",
       "      <td>0.985564</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.078349</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119888</td>\n",
       "      <td>0.325183</td>\n",
       "      <td>0.882466</td>\n",
       "      <td>0.917528</td>\n",
       "      <td>0.421067</td>\n",
       "      <td>0.563627</td>\n",
       "      <td>0.500238</td>\n",
       "      <td>0.263457</td>\n",
       "      <td>434.193115</td>\n",
       "      <td>0.981752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lowlevel.average_loudness  lowlevel.barkbands.mean_0  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.933530                   0.003060   \n",
       "0010xmHR6UICBOYT                   0.985564                   0.009209   \n",
       "\n",
       "                  lowlevel.barkbands.mean_1  lowlevel.barkbands.mean_2  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.015665                   0.004106   \n",
       "0010xmHR6UICBOYT                   0.078349                   0.004850   \n",
       "\n",
       "                  lowlevel.barkbands.mean_3  lowlevel.barkbands.mean_4  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.001265                   0.001988   \n",
       "0010xmHR6UICBOYT                   0.001458                   0.000622   \n",
       "\n",
       "                  lowlevel.barkbands.mean_5  lowlevel.barkbands.mean_6  \\\n",
       "id                                                                       \n",
       "0009fFIM1eYThaPg                   0.001152                   0.000906   \n",
       "0010xmHR6UICBOYT                   0.000331                   0.001254   \n",
       "\n",
       "                  lowlevel.barkbands.mean_7  lowlevel.barkbands.mean_8  ...  \\\n",
       "id                                                                      ...   \n",
       "0009fFIM1eYThaPg                   0.000356                   0.001064  ...   \n",
       "0010xmHR6UICBOYT                   0.003589                   0.000960  ...   \n",
       "\n",
       "                  tonal.thpcp_30  tonal.thpcp_31  tonal.thpcp_32  \\\n",
       "id                                                                 \n",
       "0009fFIM1eYThaPg        0.752843        0.544831        0.394721   \n",
       "0010xmHR6UICBOYT        0.119888        0.325183        0.882466   \n",
       "\n",
       "                  tonal.thpcp_33  tonal.thpcp_34  tonal.thpcp_35  \\\n",
       "id                                                                 \n",
       "0009fFIM1eYThaPg        0.372330        0.422420        0.855395   \n",
       "0010xmHR6UICBOYT        0.917528        0.421067        0.563627   \n",
       "\n",
       "                  tonal.tuning_diatonic_strength  \\\n",
       "id                                                 \n",
       "0009fFIM1eYThaPg                        0.563373   \n",
       "0010xmHR6UICBOYT                        0.500238   \n",
       "\n",
       "                  tonal.tuning_equal_tempered_deviation  \\\n",
       "id                                                        \n",
       "0009fFIM1eYThaPg                               0.226941   \n",
       "0010xmHR6UICBOYT                               0.263457   \n",
       "\n",
       "                  tonal.tuning_frequency  \\\n",
       "id                                         \n",
       "0009fFIM1eYThaPg              434.193115   \n",
       "0010xmHR6UICBOYT              434.193115   \n",
       "\n",
       "                  tonal.tuning_nontempered_energy_ratio  \n",
       "id                                                       \n",
       "0009fFIM1eYThaPg                               0.944264  \n",
       "0010xmHR6UICBOYT                               0.981752  \n",
       "\n",
       "[2 rows x 1034 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essentia  = dt.fread(file_essentia)\n",
    "essentia[dt.float64] = dt.float32\n",
    "essentia = essentia.to_pandas()\n",
    "essentia.set_index('id', inplace=True)\n",
    "essentia.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3493c0b-6506-44d7-9f59-f3628cec9a27",
   "metadata": {},
   "source": [
    "### [Mid-level] BLF: Block-Level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0cfa4ff-feec-49af-96d5-fb4c3e91fa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BLF: Block-Level features\n",
    "\n",
    "blf_correlation  = dt.fread(file_blf_correlation)\n",
    "blf_correlation[dt.float64] = dt.float32\n",
    "blf_correlation = blf_correlation.to_pandas()\n",
    "blf_correlation.set_index('id', inplace=True)\n",
    "\n",
    "blf_deltaspectral  = dt.fread(file_blf_deltaspectral)\n",
    "blf_deltaspectral[dt.float64] = dt.float32\n",
    "blf_deltaspectral = blf_deltaspectral.to_pandas()\n",
    "blf_deltaspectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_spectral  = dt.fread(file_blf_spectral)\n",
    "blf_spectral[dt.float64] = dt.float32\n",
    "blf_spectral = blf_spectral.to_pandas()\n",
    "blf_spectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_spectralcontrast  = dt.fread(file_blf_spectralcontrast)\n",
    "blf_spectralcontrast[dt.float64] = dt.float32\n",
    "blf_spectralcontrast = blf_spectralcontrast.to_pandas()\n",
    "blf_spectralcontrast.set_index('id', inplace=True)\n",
    "\n",
    "blf_vardeltaspectral  = dt.fread(file_blf_vardeltaspectral)\n",
    "blf_vardeltaspectral[dt.float64] = dt.float32\n",
    "blf_vardeltaspectral = blf_vardeltaspectral.to_pandas()\n",
    "blf_vardeltaspectral.set_index('id', inplace=True)\n",
    "\n",
    "blf_logfluc  = dt.fread(file_blf_logfluc)\n",
    "blf_logfluc[dt.float64] = dt.float32\n",
    "\n",
    "# This is done because in the csv it has an extra column name, \n",
    "# so in case someone with the original dataset tries to run it, it fixes that error\n",
    "# It looks weird, but it is because first i am loading the data into datatable and then pass it to dataframe\n",
    "new_cols = ['id']\n",
    "new_cols.extend(list(blf_logfluc.names[2:]))\n",
    "new_cols = tuple(new_cols)\n",
    "del blf_logfluc[:, -1]\n",
    "\n",
    "blf_logfluc.names = new_cols\n",
    "blf_logfluc = blf_logfluc.to_pandas()\n",
    "blf_logfluc.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeacc90-c862-48d6-831e-f3ff6868a0df",
   "metadata": {},
   "source": [
    "### [Mid-level] MFCC: Mel Frequency Cepstral Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8902ca71-a8d3-4336-83ec-415347a92637",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_bow  = dt.fread(file_mfcc_bow)\n",
    "mfcc_bow[dt.float64] = dt.float32\n",
    "mfcc_bow = mfcc_bow.to_pandas()\n",
    "mfcc_bow.set_index('id', inplace=True)\n",
    "\n",
    "mfcc_stats  = dt.fread(file_mfcc_stats)\n",
    "mfcc_stats[dt.float64] = dt.float32\n",
    "mfcc_stats = mfcc_stats.to_pandas()\n",
    "mfcc_stats.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af626c8-4f55-445b-a61f-7ff6baf9b16c",
   "metadata": {},
   "source": [
    "## Video features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da64fbc6-4d3a-41da-bb34-8af0aaeb5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "incp  = dt.fread(file_incp)\n",
    "incp[dt.float64] = dt.float32\n",
    "incp = incp.to_pandas()\n",
    "incp.set_index('id', inplace=True)\n",
    "\n",
    "resnet  = dt.fread(file_resnet)\n",
    "resnet[dt.float64] = dt.float32\n",
    "resnet = resnet.to_pandas()\n",
    "resnet.set_index('id', inplace=True)\n",
    "\n",
    "vgg19  = dt.fread(file_vgg19)\n",
    "vgg19[dt.float64] = dt.float32\n",
    "vgg19 = vgg19.to_pandas()\n",
    "vgg19.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9d4d7-d725-485e-a4aa-05c7dcb56423",
   "metadata": {},
   "source": [
    "# Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86b9d96-8689-4376-bef2-1f36a492fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA , KernelPCA, FastICA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectFpr\n",
    "import functools as ft\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70bc7a47-e234-42f6-8f27-5152323911f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# since to compute every set for the full dataset is to big we will take only a sample set to compute scores and check which set will be best.\\n# if we find a good set we can do it on the full dataset later. This Strategy should save some computation time.\\n# lets take 20%\\nsample_ids = info.sample(frac=0.2, random_state=101)\\nsample_ids\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# since to compute every set for the full dataset is to big we will take only a sample set to compute scores and check which set will be best.\n",
    "# if we find a good set we can do it on the full dataset later. This Strategy should save some computation time.\n",
    "# lets take 20%\n",
    "sample_ids = info.sample(frac=0.2, random_state=101)\n",
    "sample_ids\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a961840-4b6d-4ed7-bc0f-ffdb8cc5d195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# now we have sample ids and can take a sample from each dataframe. We will overwrite the original Dataset to save Memory.\\ntf_idf = tf_idf.loc[sample_ids.index.values]\\nword2vec = word2vec.loc[sample_ids.index.values]\\nbert = bert.loc[sample_ids.index.values]\\nessentia = essentia.loc[sample_ids.index.values]\\nblf_correlation = blf_correlation.loc[sample_ids.index.values]\\nblf_deltaspectral = blf_deltaspectral.loc[sample_ids.index.values]\\nblf_spectral = blf_spectral.loc[sample_ids.index.values]\\nblf_spectralcontrast = blf_spectralcontrast.loc[sample_ids.index.values]\\nblf_vardeltaspectral = blf_vardeltaspectral.loc[sample_ids.index.values]\\nblf_logfluc = blf_logfluc.loc[sample_ids.index.values]\\nmfcc_bow = mfcc_bow.loc[sample_ids.index.values]\\nmfcc_stats = mfcc_stats.loc[sample_ids.index.values]\\nincp = incp.loc[sample_ids.index.values]\\nresnet = resnet.loc[sample_ids.index.values]\\nvgg19 = vgg19.loc[sample_ids.index.values]\\n\\n\\n# just a check so each Dataframe have correct IDS\\nassert(np.all(sample_ids.index.values == tf_idf.index.values))\\nassert(np.all(sample_ids.index.values == word2vec.index.values))\\nassert(np.all(sample_ids.index.values == bert.index.values))\\nassert(np.all(sample_ids.index.values == essentia.index.values))\\nassert(np.all(sample_ids.index.values == blf_correlation.index.values))\\nassert(np.all(sample_ids.index.values == blf_deltaspectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_spectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_spectralcontrast.index.values))\\nassert(np.all(sample_ids.index.values == blf_vardeltaspectral.index.values))\\nassert(np.all(sample_ids.index.values == blf_logfluc.index.values))\\nassert(np.all(sample_ids.index.values == mfcc_bow.index.values))\\nassert(np.all(sample_ids.index.values == mfcc_stats.index.values))\\nassert(np.all(sample_ids.index.values == incp.index.values))\\nassert(np.all(sample_ids.index.values == resnet.index.values))\\nassert(np.all(sample_ids.index.values == vgg19.index.values))\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# now we have sample ids and can take a sample from each dataframe. We will overwrite the original Dataset to save Memory.\n",
    "tf_idf = tf_idf.loc[sample_ids.index.values]\n",
    "word2vec = word2vec.loc[sample_ids.index.values]\n",
    "bert = bert.loc[sample_ids.index.values]\n",
    "essentia = essentia.loc[sample_ids.index.values]\n",
    "blf_correlation = blf_correlation.loc[sample_ids.index.values]\n",
    "blf_deltaspectral = blf_deltaspectral.loc[sample_ids.index.values]\n",
    "blf_spectral = blf_spectral.loc[sample_ids.index.values]\n",
    "blf_spectralcontrast = blf_spectralcontrast.loc[sample_ids.index.values]\n",
    "blf_vardeltaspectral = blf_vardeltaspectral.loc[sample_ids.index.values]\n",
    "blf_logfluc = blf_logfluc.loc[sample_ids.index.values]\n",
    "mfcc_bow = mfcc_bow.loc[sample_ids.index.values]\n",
    "mfcc_stats = mfcc_stats.loc[sample_ids.index.values]\n",
    "incp = incp.loc[sample_ids.index.values]\n",
    "resnet = resnet.loc[sample_ids.index.values]\n",
    "vgg19 = vgg19.loc[sample_ids.index.values]\n",
    "\n",
    "\n",
    "# just a check so each Dataframe have correct IDS\n",
    "assert(np.all(sample_ids.index.values == tf_idf.index.values))\n",
    "assert(np.all(sample_ids.index.values == word2vec.index.values))\n",
    "assert(np.all(sample_ids.index.values == bert.index.values))\n",
    "assert(np.all(sample_ids.index.values == essentia.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_correlation.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_deltaspectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_spectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_spectralcontrast.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_vardeltaspectral.index.values))\n",
    "assert(np.all(sample_ids.index.values == blf_logfluc.index.values))\n",
    "assert(np.all(sample_ids.index.values == mfcc_bow.index.values))\n",
    "assert(np.all(sample_ids.index.values == mfcc_stats.index.values))\n",
    "assert(np.all(sample_ids.index.values == incp.index.values))\n",
    "assert(np.all(sample_ids.index.values == resnet.index.values))\n",
    "assert(np.all(sample_ids.index.values == vgg19.index.values))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5711b-06a2-458e-87a9-1d5ca7e2eed9",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd85bd29-85ef-4f50-a1c6-d11fb374d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA feature selection, we can get only features so we still have 99% explained variance from the Data.\n",
    "def PCA_selection(df_set):\n",
    "    pca = PCA(n_components=0.99, svd_solver='auto')\n",
    "    PCA_features = pd.DataFrame(pca.fit_transform(df_set))\n",
    "    print (f' original shape: {df_set.shape}') \n",
    "    print (f' components = {pca.n_components_}, explained variance = {pca.explained_variance_ratio_.sum()}')\n",
    "    print (f' features reduced by {(1-pca.n_components_/df_set.shape[1]) *100:.2f}%')\n",
    "    return PCA_features\n",
    "\n",
    "# We can use also PCA Kernel but we need to fix the number of components. Here we have different things to try\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’}, default=’linear’\n",
    "def PCA_Kernel_selection(df_set, kernel='rbf', components:int=100, gamma=None):\n",
    "    kernel_pca = KernelPCA(n_components=components, kernel=kernel, gamma=gamma)\n",
    "    kernel_PCA_features = pd.DataFrame(kernel_pca.fit_transform(df_set))\n",
    "    return kernel_PCA_features\n",
    "\n",
    "# Fast ICA would be another solution of dimensionality reduction\n",
    "def ICA_selection(df_set, components:int=100):\n",
    "    fast_ICA = FastICA(n_components=components)\n",
    "    fast_ICA_features = pd.DataFrame(fast_ICA.fit_transform(df_set))\n",
    "    return fast_ICA_features\n",
    "\n",
    "# we could also try TSNE, but not sure about performance. maybe long computation time\n",
    "def TSNE_selection(df_set, components:int=100):\n",
    "    tsne = TSNE(n_components=components, learning_rate='auto', init='random')\n",
    "    tsne_features = pd.DataFrame(tsne.fit_transform(mid_low_features))\n",
    "    return tsne_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4446acbf-9425-4aee-8d81-6e61e446098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features for each category\n",
    "text_features = {'tf_idf': tf_idf, 'word2vec':word2vec, 'bert':bert}\n",
    "audio_features = {'essentia':essentia,\n",
    "                  'blf_correlation':blf_correlation,\n",
    "                  'blf_deltaspectral':blf_deltaspectral,\n",
    "                  'blf_spectral':blf_spectral,\n",
    "                  'blf_spectralcontrast':blf_spectralcontrast,\n",
    "                  'blf_vardeltaspectral':blf_vardeltaspectral,\n",
    "                  'blf_logfluc':blf_logfluc,\n",
    "                  'mfcc_bow':mfcc_bow,\n",
    "                  'mfcc_stats':mfcc_stats}\n",
    "video_features = {'incp':incp, 'resnet':resnet, 'vgg19':vgg19}\n",
    "all_features = {**text_features, **audio_features, **video_features}\n",
    "\n",
    "\n",
    "# since if we want to try all different combinations this would be to much to handle. there are thousends of different combinations we need to concentrate on some.\n",
    "# from previous task we know bert and resnet where the more important features. so lets take them and combine with different audio features.\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "set1 = flatten([['bert'], audio_features.keys(), ['resnet']]) # one set with 1 text, audio, 1 video\n",
    "set2 = flatten([['bert'], audio_features.keys()]) # one set with 1 text, audio, without video\n",
    "set3 = flatten([audio_features.keys(), ['resnet']]) # one set without text, audio, video\n",
    "set4 = flatten([['bert'], ['resnet']]) # one set with text, video\n",
    "set5 = flatten([['bert'], ['essentia'], ['resnet']])  # one set with text, essentia, video\n",
    "set6 = flatten([['bert'], ['blf_correlation'], ['resnet']])\n",
    "set7 = flatten([['bert'], ['blf_deltaspectral'], ['resnet']])\n",
    "set8 = flatten([['bert'], ['essentia', 'blf_logfluc'], ['resnet']])\n",
    "set9 = flatten([['bert'], ['essentia', 'blf_logfluc', 'mfcc_bow'], ['resnet']])\n",
    "set10 = flatten([['bert'], ['essentia', 'blf_logfluc', 'mfcc_bow']])\n",
    "\n",
    "allsets = [set1, set2, set3, set4, set5, set6, set7, set8, set9, set10]\n",
    "\n",
    "# function for join sets, for later use. we are limited on RAM so do it only when we need the dataset.\n",
    "def join_dfs(set_n, features):\n",
    "    fset = [features[x] for x in set_n]\n",
    "    return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e54896-cec3-4736-af30-f86ec4be08c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already a combination of 5 features give 126 different combinations. To much to Test all!\n",
    "result_list = list(itertools.combinations(audio_features.keys(), 5))\n",
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a33f894-fee7-444b-ac05-9ea9ad2221d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf\n",
      " original shape: (68641, 1000)\n",
      " components = 957, explained variance = 0.9901756877480314\n",
      " features reduced by 4.30%\n",
      "------------------\n",
      "word2vec\n",
      " original shape: (68641, 300)\n",
      " components = 270, explained variance = 0.990314907049147\n",
      " features reduced by 10.00%\n",
      "------------------\n",
      "bert\n",
      " original shape: (68641, 768)\n",
      " components = 525, explained variance = 0.990061538059662\n",
      " features reduced by 31.64%\n",
      "------------------\n",
      "essentia\n",
      " original shape: (68641, 1034)\n",
      " components = 2, explained variance = 0.9918565334205097\n",
      " features reduced by 99.81%\n",
      "------------------\n",
      "blf_correlation\n",
      " original shape: (68641, 1326)\n",
      " components = 960, explained variance = 0.9900035135747746\n",
      " features reduced by 27.60%\n",
      "------------------\n",
      "blf_deltaspectral\n",
      " original shape: (68641, 1372)\n",
      " components = 362, explained variance = 0.9900050860046592\n",
      " features reduced by 73.62%\n",
      "------------------\n",
      "blf_spectral\n",
      " original shape: (68641, 980)\n",
      " components = 147, explained variance = 0.9901046629066191\n",
      " features reduced by 85.00%\n",
      "------------------\n",
      "blf_spectralcontrast\n",
      " original shape: (68641, 800)\n",
      " components = 108, explained variance = 0.9900852007686157\n",
      " features reduced by 86.50%\n",
      "------------------\n",
      "blf_vardeltaspectral\n",
      " original shape: (68641, 1344)\n",
      " components = 177, explained variance = 0.99005190527436\n",
      " features reduced by 86.83%\n",
      "------------------\n",
      "blf_logfluc\n",
      " original shape: (68641, 3626)\n",
      " components = 554, explained variance = 0.9900149002868229\n",
      " features reduced by 84.72%\n",
      "------------------\n",
      "mfcc_bow\n",
      " original shape: (68641, 500)\n",
      " components = 388, explained variance = 0.9900054837042523\n",
      " features reduced by 22.40%\n",
      "------------------\n",
      "mfcc_stats\n",
      " original shape: (68641, 104)\n",
      " components = 76, explained variance = 0.9903480738159525\n",
      " features reduced by 26.92%\n",
      "------------------\n",
      "incp\n",
      " original shape: (68641, 4096)\n",
      " components = 2295, explained variance = 0.9900060283806268\n",
      " features reduced by 43.97%\n",
      "------------------\n",
      "resnet\n",
      " original shape: (68641, 4096)\n",
      " components = 1946, explained variance = 0.9900125487446361\n",
      " features reduced by 52.49%\n",
      "------------------\n",
      "vgg19\n",
      " original shape: (68641, 8192)\n",
      " components = 4377, explained variance = 0.9900061208000029\n",
      " features reduced by 46.57%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# lets see how much we can reduce by PCA if we still want to have 99% explained variance:\n",
    "pca_features={}\n",
    "for name, val in zip(all_features.keys(), all_features.values()):\n",
    "    print(name)\n",
    "    pca_features[name] = PCA_selection(val)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4a412-0cc2-4c97-bd34-27384a08008f",
   "metadata": {},
   "source": [
    "## Compute similarity and top IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5613187-532e-4a05-a7d9-6fe229c13614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ids(df, setname):\n",
    "    # generate similarity\n",
    "    result = compute_in_batches_distance(df.to_numpy(dtype=np.float32), df.to_numpy(dtype=np.float32), simfunction = get_cosine_similarity, batches=100)\n",
    "    print(result.shape)\n",
    "    np.fill_diagonal(result, -1)\n",
    "    # save to file\n",
    "    dt.Frame(pd.DataFrame(compute_in_batches_topIds(result,info.index.values,100,100), index=info.index.values).reset_index()).to_csv(f'./TopIdsTaskGenerated/top_ids_cosine_earlyfusion_{name}_complete.csv')\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49fa080b-699e-43ee-bfe1-3f148fe88f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing set1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'33_x', '99_x', '109_x', '35_x', '63_x', '40_x', '83_x', '68_x', '82_x', '87_x', '25_x', '119_x', '6_x', '46_x', '12_x', '52_x', '106_x', '21_x', '78_x', '74_x', '55_x', '14_x', '145_x', '30_x', '98_x', '132_x', '81_x', '134_x', '48_x', '113_x', '42_x', '91_x', '67_x', '38_x', '45_x', '27_x', '7_x', '69_x', '80_x', '75_x', '86_x', '23_x', '144_x', '28_x', '79_x', '26_x', '135_x', '57_x', '107_x', '60_x', '61_x', '37_x', '66_x', '112_x', '96_x', '121_x', '127_x', '20_x', '102_x', '101_x', '71_x', '5_x', '41_x', '56_x', '141_x', '44_x', '58_x', '88_x', '140_x', '92_x', '16_x', '49_x', '118_x', '50_x', '10_x', '114_x', '62_x', '122_x', '142_x', '51_x', '139_x', '65_x', '125_x', '116_x', '77_x', '3_x', '24_x', '93_x', '29_x', '17_x', '47_x', '85_x', '95_x', '94_x', '110_x', '100_x', '138_x', '43_x', '120_x', '18_x', '126_x', '39_x', '54_x', '123_x', '73_x', '84_x', '2_x', '34_x', '22_x', '19_x', '64_x', '108_x', '4_x', '72_x', '128_x', '103_x', '9_x', '133_x', '8_x', '97_x', '104_x', '90_x', '117_x', '124_x', '11_x', '130_x', '36_x', '59_x', '129_x', '143_x', '115_x', '76_x', '131_x', '89_x', '32_x', '70_x', '53_x', '137_x', '13_x', '31_x', '111_x', '136_x', '146_x', '15_x', '105_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'155_x', '33_x', '99_x', '147_x', '35_x', '63_x', '40_x', '149_x', '83_x', '68_x', '82_x', '174_x', '87_x', '25_x', '6_x', '46_x', '12_x', '52_x', '106_x', '21_x', '78_x', '148_x', '55_x', '14_x', '158_x', '74_x', '30_x', '98_x', '81_x', '48_x', '175_x', '42_x', '91_x', '67_x', '38_x', '45_x', '152_x', '27_x', '156_x', '7_x', '69_x', '80_x', '75_x', '86_x', '154_x', '23_x', '167_x', '176_x', '28_x', '79_x', '26_x', '57_x', '107_x', '60_x', '172_x', '161_x', '61_x', '37_x', '66_x', '96_x', '173_x', '20_x', '102_x', '101_x', '71_x', '5_x', '41_x', '168_x', '56_x', '44_x', '58_x', '88_x', '92_x', '16_x', '49_x', '151_x', '50_x', '10_x', '62_x', '162_x', '150_x', '51_x', '166_x', '65_x', '77_x', '3_x', '24_x', '93_x', '29_x', '17_x', '164_x', '153_x', '47_x', '85_x', '95_x', '94_x', '169_x', '100_x', '43_x', '18_x', '157_x', '39_x', '54_x', '73_x', '171_x', '2_x', '84_x', '34_x', '22_x', '19_x', '64_x', '160_x', '4_x', '72_x', '170_x', '103_x', '9_x', '165_x', '8_x', '97_x', '104_x', '90_x', '159_x', '11_x', '36_x', '59_x', '76_x', '89_x', '32_x', '70_x', '53_x', '163_x', '13_x', '31_x', '15_x', '105_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'299_x', '292_x', '296_x', '345_x', '217_x', '320_x', '282_x', '109_x', '313_x', '274_x', '257_x', '195_x', '265_x', '281_x', '119_x', '216_x', '180_x', '327_x', '353_x', '311_x', '347_x', '262_x', '340_x', '225_x', '211_x', '232_x', '145_x', '302_x', '214_x', '326_x', '305_x', '132_x', '218_x', '251_x', '134_x', '256_x', '113_x', '330_x', '178_x', '185_x', '293_x', '204_x', '228_x', '144_x', '203_x', '279_x', '0_x', '280_x', '290_x', '135_x', '255_x', '210_x', '321_x', '286_x', '276_x', '333_x', '121_x', '112_x', '184_x', '192_x', '127_x', '269_x', '356_x', '271_x', '246_x', '272_x', '309_x', '331_x', '350_x', '183_x', '206_x', '193_x', '231_x', '238_x', '239_x', '335_x', '339_x', '315_x', '247_x', '197_x', '208_x', '223_x', '229_x', '177_x', '344_x', '141_x', '189_x', '253_x', '196_x', '349_x', '201_x', '264_x', '268_x', '323_x', '317_x', '343_x', '140_x', '202_x', '278_x', '118_x', '346_x', '182_x', '284_x', '114_x', '122_x', '285_x', '316_x', '207_x', '354_x', '259_x', '142_x', '298_x', '244_x', '260_x', '139_x', '236_x', '263_x', '116_x', '125_x', '209_x', '224_x', '242_x', '249_x', '261_x', '230_x', '275_x', '190_x', '325_x', '361_x', '310_x', '303_x', '258_x', '341_x', '205_x', '188_x', '110_x', '181_x', '322_x', '138_x', '240_x', '227_x', '248_x', '266_x', '120_x', '308_x', '226_x', '306_x', '312_x', '126_x', '294_x', '332_x', '338_x', '241_x', '360_x', '123_x', '307_x', '250_x', '289_x', '319_x', '336_x', '267_x', '187_x', '324_x', '222_x', '273_x', '108_x', '235_x', '270_x', '277_x', '352_x', '351_x', '128_x', '252_x', '254_x', '295_x', '314_x', '357_x', '300_x', '220_x', '348_x', '198_x', '233_x', '133_x', '237_x', '117_x', '199_x', '243_x', '245_x', '297_x', '342_x', '212_x', '301_x', '334_x', '328_x', '124_x', '291_x', '130_x', '1_x', '129_x', '221_x', '143_x', '115_x', '358_x', '186_x', '200_x', '131_x', '283_x', '287_x', '219_x', '194_x', '137_x', '337_x', '213_x', '359_x', '318_x', '111_x', '304_x', '329_x', '136_x', '215_x', '146_x', '234_x', '288_x', '191_x', '355_x', '179_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'155_x', '33_x', '99_x', '375_x', '382_x', '35_x', '63_x', '147_x', '40_x', '149_x', '83_x', '68_x', '82_x', '87_x', '174_x', '25_x', '364_x', '6_x', '46_x', '12_x', '52_x', '106_x', '21_x', '78_x', '371_x', '74_x', '55_x', '14_x', '148_x', '158_x', '367_x', '30_x', '98_x', '380_x', '81_x', '48_x', '175_x', '42_x', '91_x', '67_x', '38_x', '45_x', '152_x', '27_x', '156_x', '7_x', '69_x', '80_x', '75_x', '86_x', '154_x', '23_x', '167_x', '176_x', '28_x', '79_x', '26_x', '57_x', '374_x', '107_x', '60_x', '172_x', '161_x', '61_x', '37_x', '66_x', '96_x', '173_x', '20_x', '102_x', '365_x', '101_x', '71_x', '5_x', '373_x', '41_x', '168_x', '56_x', '387_x', '44_x', '58_x', '88_x', '92_x', '377_x', '16_x', '49_x', '151_x', '50_x', '10_x', '383_x', '62_x', '162_x', '384_x', '150_x', '51_x', '166_x', '65_x', '363_x', '381_x', '368_x', '372_x', '77_x', '362_x', '3_x', '24_x', '93_x', '29_x', '376_x', '17_x', '164_x', '47_x', '153_x', '370_x', '85_x', '95_x', '94_x', '169_x', '100_x', '43_x', '18_x', '157_x', '39_x', '54_x', '73_x', '84_x', '2_x', '171_x', '34_x', '22_x', '19_x', '64_x', '160_x', '4_x', '72_x', '170_x', '103_x', '9_x', '379_x', '8_x', '165_x', '97_x', '104_x', '90_x', '386_x', '159_x', '11_x', '36_x', '59_x', '378_x', '76_x', '89_x', '32_x', '70_x', '385_x', '53_x', '163_x', '13_x', '31_x', '366_x', '369_x', '15_x', '105_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'0_x', '1_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      "V:\\Temp\\ipykernel_47332\\2988892163.py:37: FutureWarning: Passing 'suffixes' which cause duplicate columns {'481_x', '411_x', '274_x', '439_x', '440_x', '68_x', '216_x', '327_x', '470_x', '225_x', '232_x', '74_x', '55_x', '420_x', '132_x', '480_x', '251_x', '113_x', '330_x', '503_x', '75_x', '468_x', '204_x', '144_x', '228_x', '280_x', '467_x', '135_x', '26_x', '321_x', '286_x', '37_x', '66_x', '462_x', '356_x', '272_x', '425_x', '183_x', '339_x', '247_x', '390_x', '428_x', '189_x', '196_x', '349_x', '438_x', '323_x', '343_x', '62_x', '285_x', '298_x', '490_x', '65_x', '116_x', '224_x', '436_x', '408_x', '190_x', '494_x', '518_x', '24_x', '258_x', '499_x', '413_x', '138_x', '240_x', '457_x', '227_x', '120_x', '306_x', '312_x', '294_x', '453_x', '360_x', '424_x', '73_x', '456_x', '250_x', '319_x', '22_x', '64_x', '478_x', '128_x', '357_x', '348_x', '198_x', '394_x', '401_x', '414_x', '433_x', '117_x', '398_x', '243_x', '400_x', '515_x', '124_x', '212_x', '328_x', '291_x', '36_x', '129_x', '143_x', '200_x', '287_x', '406_x', '451_x', '53_x', '137_x', '31_x', '442_x', '318_x', '304_x', '329_x', '215_x', '234_x', '191_x', '355_x', '502_x', '299_x', '292_x', '296_x', '345_x', '109_x', '63_x', '40_x', '257_x', '449_x', '498_x', '353_x', '479_x', '52_x', '46_x', '262_x', '340_x', '211_x', '21_x', '145_x', '485_x', '475_x', '305_x', '256_x', '452_x', '38_x', '178_x', '27_x', '69_x', '23_x', '389_x', '28_x', '458_x', '472_x', '333_x', '61_x', '184_x', '269_x', '271_x', '331_x', '239_x', '231_x', '335_x', '20_x', '434_x', '501_x', '197_x', '71_x', '5_x', '41_x', '344_x', '441_x', '429_x', '511_x', '141_x', '56_x', '264_x', '388_x', '268_x', '392_x', '317_x', '444_x', '10_x', '510_x', '114_x', '207_x', '516_x', '259_x', '260_x', '236_x', '125_x', '263_x', '464_x', '242_x', '230_x', '275_x', '3_x', '310_x', '17_x', '459_x', '341_x', '205_x', '188_x', '248_x', '266_x', '506_x', '18_x', '332_x', '338_x', '307_x', '34_x', '446_x', '324_x', '72_x', '235_x', '270_x', '277_x', '352_x', '252_x', '254_x', '466_x', '245_x', '297_x', '11_x', '423_x', '186_x', '131_x', '213_x', '13_x', '445_x', '513_x', '448_x', '217_x', '320_x', '509_x', '25_x', '281_x', '311_x', '426_x', '500_x', '517_x', '347_x', '302_x', '214_x', '134_x', '477_x', '417_x', '497_x', '461_x', '455_x', '185_x', '293_x', '489_x', '435_x', '203_x', '279_x', '512_x', '255_x', '210_x', '514_x', '246_x', '350_x', '391_x', '523_x', '405_x', '223_x', '208_x', '229_x', '491_x', '58_x', '416_x', '201_x', '118_x', '278_x', '488_x', '182_x', '122_x', '354_x', '51_x', '427_x', '407_x', '244_x', '431_x', '471_x', '249_x', '395_x', '261_x', '507_x', '29_x', '492_x', '110_x', '521_x', '181_x', '322_x', '493_x', '519_x', '43_x', '308_x', '39_x', '396_x', '19_x', '336_x', '273_x', '108_x', '4_x', '351_x', '469_x', '295_x', '412_x', '474_x', '9_x', '233_x', '133_x', '334_x', '301_x', '415_x', '59_x', '486_x', '221_x', '358_x', '419_x', '70_x', '359_x', '111_x', '136_x', '418_x', '146_x', '288_x', '179_x', '476_x', '33_x', '496_x', '282_x', '409_x', '313_x', '422_x', '35_x', '505_x', '195_x', '265_x', '447_x', '119_x', '180_x', '6_x', '12_x', '520_x', '14_x', '326_x', '30_x', '437_x', '218_x', '48_x', '42_x', '67_x', '45_x', '7_x', '402_x', '450_x', '290_x', '57_x', '60_x', '121_x', '276_x', '112_x', '127_x', '192_x', '309_x', '206_x', '473_x', '193_x', '238_x', '410_x', '315_x', '487_x', '443_x', '177_x', '522_x', '44_x', '253_x', '140_x', '16_x', '49_x', '202_x', '50_x', '346_x', '284_x', '316_x', '142_x', '139_x', '432_x', '209_x', '325_x', '361_x', '495_x', '524_x', '303_x', '430_x', '47_x', '463_x', '393_x', '397_x', '226_x', '126_x', '482_x', '123_x', '241_x', '54_x', '2_x', '404_x', '289_x', '267_x', '187_x', '222_x', '421_x', '484_x', '454_x', '460_x', '314_x', '300_x', '220_x', '8_x', '237_x', '508_x', '504_x', '199_x', '465_x', '342_x', '130_x', '483_x', '403_x', '115_x', '399_x', '283_x', '32_x', '219_x', '194_x', '337_x', '15_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), fset)\n",
      " 66%|██████▌   | 66/100 [01:43<00:53,  1.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoing set\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m joined_df \u001b[38;5;241m=\u001b[39m join_dfs(s, pca_features)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mgenerate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mset\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mgenerate_ids\u001b[1;34m(df, setname)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_ids\u001b[39m(df, setname):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# generate similarity\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_in_batches_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimfunction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_cosine_similarity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m     np\u001b[38;5;241m.\u001b[39mfill_diagonal(result, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Google Drive\\Study\\5th Semester\\Multimedia Search and Retrieval\\Task 1\\Git\\MMSR\\project\\functions.py:43\u001b[0m, in \u001b[0;36mcompute_in_batches_distance\u001b[1;34m(arr_a, arr_b, simfunction, batches)\u001b[0m\n\u001b[0;32m     41\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m tqdm(splits_b):\n\u001b[1;32m---> 43\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(\u001b[43msimfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(r, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Google Drive\\Study\\5th Semester\\Multimedia Search and Retrieval\\Task 1\\Git\\MMSR\\project\\functions.py:17\u001b[0m, in \u001b[0;36mget_cosine_similarity\u001b[1;34m(arr_a, arr_b)\u001b[0m\n\u001b[0;32m     15\u001b[0m norms_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(arr_b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m     16\u001b[0m divisor \u001b[38;5;241m=\u001b[39m norms_a \u001b[38;5;241m*\u001b[39m norms_b\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m---> 17\u001b[0m dotp \u001b[38;5;241m=\u001b[39m \u001b[43marr_a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marr_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[0;32m     18\u001b[0m r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(dotp, divisor, np\u001b[38;5;241m.\u001b[39mzeros_like(divisor), where\u001b[38;5;241m=\u001b[39mdivisor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run for all sets\n",
    "for i, s in enumerate(allsets):\n",
    "    print(f'doing set{i+1}')\n",
    "    joined_df = join_dfs(s, pca_features)\n",
    "    generate_ids(joined_df, f'set{i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56505da6-4f67-40d1-a855-3aef79b47ab7",
   "metadata": {},
   "source": [
    "## Compute NDCG score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d148f55d-9a3d-42e4-bf32-81506f1c0789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_set1 = dt.fread('./TopIdsTaskGenerated/top_ids_cosine_earlyfusion_set1_complete.csv', header=True).to_pandas()\n",
    "top_set1.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7cae6b6b-2c7d-435f-977b-44d08d3dcc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68641/68641 [02:51<00:00, 400.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single function (0.468423082690376, 0.6038665277735273, 0.7019879294807087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Single function\", getMetrics(top_set1, 100, genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e11df4-e3e7-488e-88e3-d859065d7674",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fad43af-6228-4f97-9c03-83e443ee66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def normalize_df(df, post_processing_scaler):\n",
    "    df_np_values = df.values  #returns a numpy array\n",
    "    # scale\n",
    "    df_normalized = post_processing_scaler.fit_transform(df_np_values)\n",
    "    # reset index from original\n",
    "    df_normalized = pd.DataFrame(df_normalized).set_index(df.index)\n",
    "    df_normalized.columns = df.columns[:]\n",
    "    return df_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
